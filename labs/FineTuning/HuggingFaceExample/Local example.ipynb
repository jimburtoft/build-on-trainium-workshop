{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "This notebook lets you run the training and inference on the local EC2 instance instead of the other notebooks that use SageMaker.\n",
    "In the AWS hosted workshop environment, for your kernel pick the Python kernel aws_neuronx_venv_pytorch_latest.\n",
    "In your own environment, deploy the Neuron DLAMI and choose the nxd_inference kernel in /opt.  Also update your path in the cd command below.\n",
    "\n",
    "# Install requirements\n",
    "\n",
    "We'll use the same files as the SageMaker training, so we'll first move to the assets directory and run our scripts from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/environment/FineTuning/HuggingFaceExample/01_finetuning/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /home/ubuntu/environment/FineTuning/HuggingFaceExample/01_finetuning/assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "We will use the same training scripts as we do in the SageMaker examples, we just need to launch them with the torchrun process and the same parameters that we would have passed in.  See the Finetune-TinyLlama-1.1B notebook for more information on the parameters.\n",
    "\n",
    "Additionally, this example uses a Qwen model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0904 12:53:49.952000 249259 torch/distributed/run.py:766] \n",
      "W0904 12:53:49.952000 249259 torch/distributed/run.py:766] *****************************************\n",
      "W0904 12:53:49.952000 249259 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0904 12:53:49.952000 249259 torch/distributed/run.py:766] *****************************************\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "2025-09-04 12:54:07.681480: W neuron/nrt_adaptor.cc:53] nrt_tensor_write_hugepage() is not available, will fall back to nrt_tensor_write().\n",
      "2025-09-04 12:54:07.681514: W neuron/nrt_adaptor.cc:62] nrt_tensor_read_hugepage() is not available, will fall back to nrt_tensor_read().\n",
      "2025-09-04 12:54:07.682566: W neuron/nrt_adaptor.cc:53] nrt_tensor_write_hugepage() is not available, will fall back to nrt_tensor_write().\n",
      "2025-09-04 12:54:07.682596: W neuron/nrt_adaptor.cc:62] nrt_tensor_read_hugepage() is not available, will fall back to nrt_tensor_read().\n",
      "2025-Sep-04 12:54:07.0683 249280:249344 [0] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):213 CCOM WARN NET/OFI Failed to initialize sendrecv protocol\n",
      "2025-Sep-04 12:54:07.0684 249281:249345 [1] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):213 CCOM WARN NET/OFI Failed to initialize sendrecv protocol\n",
      "2025-Sep-04 12:54:07.0684 249280:249344 [0] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):354 CCOM WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "2025-Sep-04 12:54:07.0686 249281:249345 [1] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):354 CCOM WARN NET/OFI aws-ofi-nccl initialization failed\n",
      "2025-Sep-04 12:54:07.0687 249280:249344 [0] ncclResult_t nccl_net_ofi_init_no_atexit_fini_v6(ncclDebugLogger_t):183 CCOM WARN NET/OFI Initializing plugin failed\n",
      "2025-Sep-04 12:54:07.0689 249281:249345 [1] ncclResult_t nccl_net_ofi_init_no_atexit_fini_v6(ncclDebugLogger_t):183 CCOM WARN NET/OFI Initializing plugin failed\n",
      "2025-Sep-04 12:54:07.0690 249280:249344 [0] net_plugin.cc:97 CCOM WARN OFI plugin initNet() failed is EFA enabled?\n",
      "2025-Sep-04 12:54:07.0691 249281:249345 [1] net_plugin.cc:97 CCOM WARN OFI plugin initNet() failed is EFA enabled?\n",
      "[2025-09-04 12:54:07.762: I neuronx_distributed/parallel_layers/parallel_state.py:628] > initializing tensor model parallel with size 2\n",
      "[2025-09-04 12:54:07.762: I neuronx_distributed/parallel_layers/parallel_state.py:629] > initializing pipeline model parallel with size 1\n",
      "[2025-09-04 12:54:07.762: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing context model parallel with size 1\n",
      "[2025-09-04 12:54:07.762: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing data parallel with size 1\n",
      "[2025-09-04 12:54:07.762: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing world size to 2\n",
      "2025-09-04 12:54:07.000846:  249281  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_910320559639473753+e30acd3a/model.neff\n",
      "2025-09-04 12:54:07.000846:  249280  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_910320559639473753+e30acd3a/model.neff\n",
      "[2025-09-04 12:54:07.890: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x747453e35240>, 'Ascending Ring PG Group')>\n",
      "[2025-09-04 12:54:07.891: I neuronx_distributed/parallel_layers/parallel_state.py:668] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1]]\n",
      "[2025-09-04 12:54:07.892: I neuronx_distributed/parallel_layers/parallel_state.py:669] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1]]\n",
      "[2025-09-04 12:54:07.892: I neuronx_distributed/parallel_layers/parallel_state.py:670] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1]]\n",
      "[2025-09-04 12:54:07.892: I neuronx_distributed/parallel_layers/parallel_state.py:671] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1]]\n",
      "[2025-09-04 12:54:07.892: I neuronx_distributed/parallel_layers/parallel_state.py:672] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1]]\n",
      "[2025-09-04 12:54:07.892: I neuronx_distributed/parallel_layers/parallel_state.py:673] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1]]\n",
      "torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "No Hugging Face token found in environment, checking AWS Secrets Manager...\n",
      "torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "No Hugging Face token found in environment, checking AWS Secrets Manager...\n",
      "Logging in to Hugging Face Hub...\n",
      "Logging in to Hugging Face Hub...\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/optimum/neuron/models/inference/backend/modules/kvcache/kv_cache_manager.py:24: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from ..attention.gqa import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/optimum/neuron/models/inference/backend/modules/kvcache/kv_cache_manager.py:24: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from ..attention.gqa import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/optimum/neuron/models/inference/llama/modeling_llama.py:45: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from ..backend.modules.attention.attention_base import NeuronAttentionBase\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/optimum/neuron/models/inference/llama/modeling_llama.py:45: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from ..backend.modules.attention.attention_base import NeuronAttentionBase\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.25it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.28it/s]\n",
      "torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "torch.distributed process group is initialized, but parallel_mode != ParallelMode.DISTRIBUTED. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/transformers/training_args.py:2058: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/transformers/training_args.py:2058: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/optimum/neuron/trainers.py:1467: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `NeuronSFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer.__init__(self, *args, **kwargs)\n",
      "No label_names provided for model class `NeuronPeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/optimum/neuron/trainers.py:1726: UserWarning: You passed `packing=True` to the SFTTrainer/SFTConfig, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/optimum/neuron/trainers.py:1467: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `NeuronSFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer.__init__(self, *args, **kwargs)\n",
      "No label_names provided for model class `NeuronPeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/optimum/neuron/trainers.py:1726: UserWarning: You passed `packing=True` to the SFTTrainer/SFTConfig, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5,532\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Total optimization steps = 1,000\n",
      "  Number of trainable parameters = 16,515,072\n",
      "2025-09-04 12:54:25.000231:  249280  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_14152517080381013961+a3455b04/model.neff\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "2025-09-04 12:54:25.000266:  249281  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_14152517080381013961+a3455b04/model.neff\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:485: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:485: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "2025-09-04 12:54:27.000812:  249281  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_14770574952780443587+a3455b04/model.neff\n",
      "2025-09-04 12:54:27.000890:  249280  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_13340102934497073113+a3455b04/model.neff\n",
      "2025-09-04 12:54:33.000604:  249281  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_4493465029485591167+a3455b04/model.neff\n",
      "2025-09-04 12:54:33.000704:  249280  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_6394509931854585423+a3455b04/model.neff\n",
      "2025-09-04 12:54:42.000800:  249281  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_5391683354399705816+a3455b04/model.neff\n",
      "2025-09-04 12:54:43.000072:  249280  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_17239445601988651504+a3455b04/model.neff\n",
      "2025-09-04 12:54:49.000786:  249280  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_3188978248012181906+a3455b04/model.neff\n",
      "2025-09-04 12:54:50.000270:  249281  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_3188978248012181906+a3455b04/model.neff\n",
      "2025-09-04 12:54:53.000743:  249281  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_9123409164792130896+a3455b04/model.neff\n",
      "2025-09-04 12:54:53.000781:  249280  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_9123409164792130896+a3455b04/model.neff\n",
      "2025-09-04 12:55:04.000271:  249280  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_9654582282420716943+a3455b04/model.neff\n",
      "2025-09-04 12:55:04.000400:  249281  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_9654582282420716943+a3455b04/model.neff\n",
      "{'loss': 1.7589, 'learning_rate': 4.9500000000000004e-05, 'grad_norm': 1.375, 'epoch': 0.0036153289949385392}\n",
      "{'loss': 1.1141, 'learning_rate': 4.9e-05, 'grad_norm': 0.55078125, 'epoch': 0.0072306579898770785}\n",
      "{'loss': 0.9875, 'learning_rate': 4.85e-05, 'grad_norm': 0.41796875, 'epoch': 0.010845986984815618}\n",
      "{'loss': 0.8968, 'learning_rate': 4.8e-05, 'grad_norm': 0.376953125, 'epoch': 0.014461315979754157}\n",
      "{'loss': 0.8117, 'learning_rate': 4.75e-05, 'grad_norm': 0.33984375, 'epoch': 0.018076644974692697}\n",
      "{'loss': 0.8036, 'learning_rate': 4.7e-05, 'grad_norm': 0.455078125, 'epoch': 0.021691973969631236}\n",
      "{'loss': 0.7604, 'learning_rate': 4.6500000000000005e-05, 'grad_norm': 0.4140625, 'epoch': 0.025307302964569775}\n",
      "{'loss': 0.7523, 'learning_rate': 4.600000000000001e-05, 'grad_norm': 0.44140625, 'epoch': 0.028922631959508314}\n",
      "{'loss': 0.734, 'learning_rate': 4.55e-05, 'grad_norm': 0.48046875, 'epoch': 0.03253796095444685}\n",
      "{'loss': 0.6802, 'learning_rate': 4.5e-05, 'grad_norm': 0.4765625, 'epoch': 0.036153289949385395}\n",
      "{'loss': 0.69, 'learning_rate': 4.4500000000000004e-05, 'grad_norm': 0.49609375, 'epoch': 0.03976861894432393}\n",
      "{'loss': 0.6929, 'learning_rate': 4.4000000000000006e-05, 'grad_norm': 0.5, 'epoch': 0.04338394793926247}\n",
      "{'loss': 0.6719, 'learning_rate': 4.35e-05, 'grad_norm': 0.470703125, 'epoch': 0.046999276934201015}\n",
      "{'loss': 0.6586, 'learning_rate': 4.3e-05, 'grad_norm': 0.46484375, 'epoch': 0.05061460592913955}\n",
      "{'loss': 0.6836, 'learning_rate': 4.25e-05, 'grad_norm': 0.53125, 'epoch': 0.05422993492407809}\n",
      "{'loss': 0.7023, 'learning_rate': 4.2e-05, 'grad_norm': 0.5546875, 'epoch': 0.05784526391901663}\n",
      "{'loss': 0.6744, 'learning_rate': 4.15e-05, 'grad_norm': 0.4765625, 'epoch': 0.06146059291395517}\n",
      "{'loss': 0.6754, 'learning_rate': 4.1e-05, 'grad_norm': 0.49609375, 'epoch': 0.0650759219088937}\n",
      "{'loss': 0.6554, 'learning_rate': 4.05e-05, 'grad_norm': 0.478515625, 'epoch': 0.06869125090383225}\n",
      "{'loss': 0.6459, 'learning_rate': 4e-05, 'grad_norm': 0.484375, 'epoch': 0.07230657989877079}\n",
      "{'loss': 0.6305, 'learning_rate': 3.9500000000000005e-05, 'grad_norm': 0.55859375, 'epoch': 0.07592190889370933}\n",
      "{'loss': 0.6645, 'learning_rate': 3.9000000000000006e-05, 'grad_norm': 0.80859375, 'epoch': 0.07953723788864786}\n",
      "{'loss': 0.6399, 'learning_rate': 3.85e-05, 'grad_norm': 0.5, 'epoch': 0.08315256688358641}\n",
      "{'loss': 0.6122, 'learning_rate': 3.8e-05, 'grad_norm': 0.55859375, 'epoch': 0.08676789587852494}\n",
      "{'loss': 0.6024, 'learning_rate': 3.7500000000000003e-05, 'grad_norm': 0.52734375, 'epoch': 0.09038322487346348}\n",
      "{'loss': 0.6132, 'learning_rate': 3.7e-05, 'grad_norm': 0.546875, 'epoch': 0.09399855386840203}\n",
      "{'loss': 0.6194, 'learning_rate': 3.65e-05, 'grad_norm': 0.5, 'epoch': 0.09761388286334056}\n",
      "{'loss': 0.6223, 'learning_rate': 3.6e-05, 'grad_norm': 0.55859375, 'epoch': 0.1012292118582791}\n",
      "{'loss': 0.6218, 'learning_rate': 3.55e-05, 'grad_norm': 0.5859375, 'epoch': 0.10484454085321765}\n",
      "{'loss': 0.6482, 'learning_rate': 3.5e-05, 'grad_norm': 0.53125, 'epoch': 0.10845986984815618}\n",
      "{'loss': 0.598, 'learning_rate': 3.45e-05, 'grad_norm': 0.5234375, 'epoch': 0.11207519884309472}\n",
      "{'loss': 0.6297, 'learning_rate': 3.4000000000000007e-05, 'grad_norm': 0.62109375, 'epoch': 0.11569052783803326}\n",
      "{'loss': 0.607, 'learning_rate': 3.35e-05, 'grad_norm': 0.56640625, 'epoch': 0.1193058568329718}\n",
      "{'loss': 0.6258, 'learning_rate': 3.3e-05, 'grad_norm': 0.5546875, 'epoch': 0.12292118582791034}\n",
      "{'loss': 0.6156, 'learning_rate': 3.2500000000000004e-05, 'grad_norm': 0.63671875, 'epoch': 0.1265365148228489}\n",
      "{'loss': 0.6157, 'learning_rate': 3.2000000000000005e-05, 'grad_norm': 0.5546875, 'epoch': 0.1301518438177874}\n",
      "{'loss': 0.6201, 'learning_rate': 3.15e-05, 'grad_norm': 0.4921875, 'epoch': 0.13376717281272596}\n",
      "{'loss': 0.5866, 'learning_rate': 3.1e-05, 'grad_norm': 0.7265625, 'epoch': 0.1373825018076645}\n",
      "{'loss': 0.5913, 'learning_rate': 3.05e-05, 'grad_norm': 0.59765625, 'epoch': 0.14099783080260303}\n",
      "{'loss': 0.5931, 'learning_rate': 3e-05, 'grad_norm': 0.62890625, 'epoch': 0.14461315979754158}\n",
      "{'loss': 0.6094, 'learning_rate': 2.95e-05, 'grad_norm': 0.640625, 'epoch': 0.14822848879248013}\n",
      "{'loss': 0.6037, 'learning_rate': 2.9e-05, 'grad_norm': 0.57421875, 'epoch': 0.15184381778741865}\n",
      "{'loss': 0.6133, 'learning_rate': 2.8499999999999998e-05, 'grad_norm': 0.5859375, 'epoch': 0.1554591467823572}\n",
      "{'loss': 0.5938, 'learning_rate': 2.8000000000000003e-05, 'grad_norm': 0.59375, 'epoch': 0.15907447577729572}\n",
      "{'loss': 0.6115, 'learning_rate': 2.7500000000000004e-05, 'grad_norm': 0.69140625, 'epoch': 0.16268980477223427}\n",
      "{'loss': 0.5956, 'learning_rate': 2.7000000000000002e-05, 'grad_norm': 0.6640625, 'epoch': 0.16630513376717282}\n",
      "{'loss': 0.5798, 'learning_rate': 2.6500000000000004e-05, 'grad_norm': 0.62109375, 'epoch': 0.16992046276211134}\n",
      "{'loss': 0.6057, 'learning_rate': 2.6000000000000002e-05, 'grad_norm': 0.74609375, 'epoch': 0.1735357917570499}\n",
      "{'loss': 0.6069, 'learning_rate': 2.5500000000000003e-05, 'grad_norm': 0.59375, 'epoch': 0.17715112075198844}\n",
      "{'loss': 0.5835, 'learning_rate': 2.5e-05, 'grad_norm': 0.66015625, 'epoch': 0.18076644974692696}\n",
      "Saving model checkpoint to /home/ubuntu/environment/ml/qwen/checkpoint-500\n",
      "Model parallelism is enabled, saving the model sharded state dict instead of the full state dict.\n",
      "[2025-09-04 13:03:30.565: I neuronx_distributed/trainer/checkpoint.py:144] synced saving of checkpoint adapter_shards began\n",
      "[2025-09-04 13:03:30.752: I neuronx_distributed/trainer/checkpoint.py:192] synced saving of checkpoint adapter_shards completed\n",
      "[2025-09-04 13:03:30.753: I neuronx_distributed/trainer/checkpoint.py:256] no checkpoints to remove.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:485: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:485: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "{'loss': 0.6279, 'learning_rate': 2.45e-05, 'grad_norm': 0.69921875, 'epoch': 0.1843817787418655}\n",
      "{'loss': 0.5729, 'learning_rate': 2.4e-05, 'grad_norm': 0.59375, 'epoch': 0.18799710773680406}\n",
      "{'loss': 0.5851, 'learning_rate': 2.35e-05, 'grad_norm': 0.703125, 'epoch': 0.19161243673174258}\n",
      "{'loss': 0.6042, 'learning_rate': 2.3000000000000003e-05, 'grad_norm': 0.58984375, 'epoch': 0.19522776572668113}\n",
      "{'loss': 0.5612, 'learning_rate': 2.25e-05, 'grad_norm': 0.5859375, 'epoch': 0.19884309472161968}\n",
      "{'loss': 0.5727, 'learning_rate': 2.2000000000000003e-05, 'grad_norm': 0.7109375, 'epoch': 0.2024584237165582}\n",
      "{'loss': 0.5868, 'learning_rate': 2.15e-05, 'grad_norm': 0.64453125, 'epoch': 0.20607375271149675}\n",
      "{'loss': 0.5552, 'learning_rate': 2.1e-05, 'grad_norm': 0.6484375, 'epoch': 0.2096890817064353}\n",
      "{'loss': 0.5953, 'learning_rate': 2.05e-05, 'grad_norm': 0.63671875, 'epoch': 0.21330441070137382}\n",
      "{'loss': 0.589, 'learning_rate': 2e-05, 'grad_norm': 0.578125, 'epoch': 0.21691973969631237}\n",
      "{'loss': 0.5712, 'learning_rate': 1.9500000000000003e-05, 'grad_norm': 0.62890625, 'epoch': 0.2205350686912509}\n",
      "{'loss': 0.6063, 'learning_rate': 1.9e-05, 'grad_norm': 0.66015625, 'epoch': 0.22415039768618944}\n",
      "{'loss': 0.5976, 'learning_rate': 1.85e-05, 'grad_norm': 0.65234375, 'epoch': 0.227765726681128}\n",
      "{'loss': 0.5681, 'learning_rate': 1.8e-05, 'grad_norm': 0.66796875, 'epoch': 0.2313810556760665}\n",
      "{'loss': 0.5727, 'learning_rate': 1.75e-05, 'grad_norm': 0.56640625, 'epoch': 0.23499638467100506}\n",
      "{'loss': 0.5796, 'learning_rate': 1.7000000000000003e-05, 'grad_norm': 0.77734375, 'epoch': 0.2386117136659436}\n",
      "{'loss': 0.6014, 'learning_rate': 1.65e-05, 'grad_norm': 0.703125, 'epoch': 0.24222704266088213}\n",
      "{'loss': 0.608, 'learning_rate': 1.6000000000000003e-05, 'grad_norm': 0.59765625, 'epoch': 0.24584237165582068}\n",
      "{'loss': 0.5956, 'learning_rate': 1.55e-05, 'grad_norm': 0.76953125, 'epoch': 0.24945770065075923}\n",
      "{'loss': 0.6057, 'learning_rate': 1.5e-05, 'grad_norm': 0.625, 'epoch': 0.2530730296456978}\n",
      "{'loss': 0.583, 'learning_rate': 1.45e-05, 'grad_norm': 0.65625, 'epoch': 0.25668835864063627}\n",
      "{'loss': 0.5922, 'learning_rate': 1.4000000000000001e-05, 'grad_norm': 0.8046875, 'epoch': 0.2603036876355748}\n",
      "{'loss': 0.5784, 'learning_rate': 1.3500000000000001e-05, 'grad_norm': 0.67578125, 'epoch': 0.26391901663051337}\n",
      "{'loss': 0.5647, 'learning_rate': 1.3000000000000001e-05, 'grad_norm': 0.75390625, 'epoch': 0.2675343456254519}\n",
      "{'loss': 0.5591, 'learning_rate': 1.25e-05, 'grad_norm': 0.64453125, 'epoch': 0.27114967462039047}\n",
      "{'loss': 0.5794, 'learning_rate': 1.2e-05, 'grad_norm': 0.62109375, 'epoch': 0.274765003615329}\n",
      "{'loss': 0.5825, 'learning_rate': 1.1500000000000002e-05, 'grad_norm': 0.703125, 'epoch': 0.2783803326102675}\n",
      "{'loss': 0.626, 'learning_rate': 1.1000000000000001e-05, 'grad_norm': 0.828125, 'epoch': 0.28199566160520606}\n",
      "{'loss': 0.5751, 'learning_rate': 1.05e-05, 'grad_norm': 0.58984375, 'epoch': 0.2856109906001446}\n",
      "{'loss': 0.5866, 'learning_rate': 1e-05, 'grad_norm': 0.6328125, 'epoch': 0.28922631959508316}\n",
      "{'loss': 0.543, 'learning_rate': 9.5e-06, 'grad_norm': 0.58984375, 'epoch': 0.2928416485900217}\n",
      "{'loss': 0.5908, 'learning_rate': 9e-06, 'grad_norm': 0.6484375, 'epoch': 0.29645697758496026}\n",
      "{'loss': 0.596, 'learning_rate': 8.500000000000002e-06, 'grad_norm': 0.671875, 'epoch': 0.30007230657989875}\n",
      "{'loss': 0.5656, 'learning_rate': 8.000000000000001e-06, 'grad_norm': 0.69140625, 'epoch': 0.3036876355748373}\n",
      "{'loss': 0.5717, 'learning_rate': 7.5e-06, 'grad_norm': 0.7109375, 'epoch': 0.30730296456977585}\n",
      "{'loss': 0.552, 'learning_rate': 7.000000000000001e-06, 'grad_norm': 0.61328125, 'epoch': 0.3109182935647144}\n",
      "{'loss': 0.5696, 'learning_rate': 6.5000000000000004e-06, 'grad_norm': 0.64453125, 'epoch': 0.31453362255965295}\n",
      "{'loss': 0.5694, 'learning_rate': 6e-06, 'grad_norm': 0.71875, 'epoch': 0.31814895155459144}\n",
      "{'loss': 0.5799, 'learning_rate': 5.500000000000001e-06, 'grad_norm': 0.6484375, 'epoch': 0.32176428054953}\n",
      "{'loss': 0.5572, 'learning_rate': 5e-06, 'grad_norm': 0.70703125, 'epoch': 0.32537960954446854}\n",
      "{'loss': 0.5815, 'learning_rate': 4.5e-06, 'grad_norm': 0.6953125, 'epoch': 0.3289949385394071}\n",
      "{'loss': 0.5654, 'learning_rate': 4.000000000000001e-06, 'grad_norm': 0.65625, 'epoch': 0.33261026753434564}\n",
      "{'loss': 0.5745, 'learning_rate': 3.5000000000000004e-06, 'grad_norm': 0.69140625, 'epoch': 0.3362255965292842}\n",
      "{'loss': 0.572, 'learning_rate': 3e-06, 'grad_norm': 0.75390625, 'epoch': 0.3398409255242227}\n",
      "{'loss': 0.577, 'learning_rate': 2.5e-06, 'grad_norm': 0.62109375, 'epoch': 0.34345625451916123}\n",
      "{'loss': 0.5559, 'learning_rate': 2.0000000000000003e-06, 'grad_norm': 0.66796875, 'epoch': 0.3470715835140998}\n",
      "{'loss': 0.5726, 'learning_rate': 1.5e-06, 'grad_norm': 0.671875, 'epoch': 0.35068691250903833}\n",
      "{'loss': 0.5606, 'learning_rate': 1.0000000000000002e-06, 'grad_norm': 0.640625, 'epoch': 0.3543022415039769}\n",
      "{'loss': 0.6042, 'learning_rate': 5.000000000000001e-07, 'grad_norm': 0.6328125, 'epoch': 0.3579175704989154}\n",
      "{'loss': 0.5545, 'learning_rate': 0.0, 'grad_norm': 0.66796875, 'epoch': 0.3615328994938539}\n",
      "Saving model checkpoint to /home/ubuntu/environment/ml/qwen/checkpoint-1000\n",
      "Model parallelism is enabled, saving the model sharded state dict instead of the full state dict.\n",
      "[2025-09-04 13:12:05.863: I neuronx_distributed/trainer/checkpoint.py:144] synced saving of checkpoint adapter_shards began\n",
      "[2025-09-04 13:12:06.065: I neuronx_distributed/trainer/checkpoint.py:192] synced saving of checkpoint adapter_shards completed\n",
      "[2025-09-04 13:12:06.066: I neuronx_distributed/trainer/checkpoint.py:256] no checkpoints to remove.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1061.3532, 'train_samples_per_second': 1.884, 'train_steps_per_second': 0.942, 'train_loss': 0.6340726699829101, 'epoch': 0.3615328994938539}\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "Consolidating LoRA adapter shards\n",
      "Merging LoRA adapter shards into base model\n",
      "Saving merged model to /home/ubuntu/environment/ml/qwen/merged_model\n",
      "Saving tokenizer to /home/ubuntu/environment/ml/qwen/merged_model\n",
      "Merged model config:\n",
      "Qwen3Model(\n",
      "  (embed_tokens): Embedding(151936, 2048)\n",
      "  (layers): ModuleList(\n",
      "    (0-27): 28 x Qwen3DecoderLayer(\n",
      "      (self_attn): Qwen3Attention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "      )\n",
      "      (mlp): Qwen3MLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
      "        (down_proj): Linear(in_features=6144, out_features=2048, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "      (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "    )\n",
      "  )\n",
      "  (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "  (rotary_emb): Qwen3RotaryEmbedding()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nnodes 1 --nproc_per_node 2 \\\n",
    "finetune_llama.py \\\n",
    "--bf16 True --dataloader_drop_last True --disable_tqdm True --gradient_accumulation_steps 1 \\\n",
    "--gradient_checkpointing True --learning_rate 5e-05 --logging_steps 10 --lora_alpha 32 \\\n",
    "--lora_dropout 0.05 --lora_r 16 --max_steps 1000 \\\n",
    "--model_id Qwen/Qwen3-1.7B --output_dir ~/environment/ml/qwen \\\n",
    "--per_device_train_batch_size 2 --tensor_parallel_size 2 \\\n",
    "--tokenizer_id Qwen/Qwen3-1.7B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilation\n",
    "\n",
    "Since we have everything installed locally, we don't need to use a training job like on SageMaker.  We can just call the optimum-cli command directly.\n",
    "\n",
    "The training process runs a merge script at the end, so we are using the output_dir and adding a merged_model path and then saving our compiled model into the compiled_model path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/commands/env.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import get_distribution\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:42: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/commands/env.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import get_distribution\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/neuron/models/inference/backend/modules/kvcache/kv_cache_manager.py:24: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from ..attention.gqa import (\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/neuron/models/inference/llama/modeling_llama.py:45: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from ..backend.modules.attention.attention_base import NeuronAttentionBase\n",
      "INFO:Neuron:Generating HLOs for the following models: ['context_encoding_model', 'token_generation_model']\n",
      "[2025-09-04 13:24:24.405: I neuronx_distributed/parallel_layers/parallel_state.py:628] > initializing tensor model parallel with size 2\n",
      "[2025-09-04 13:24:24.405: I neuronx_distributed/parallel_layers/parallel_state.py:629] > initializing pipeline model parallel with size 1\n",
      "[2025-09-04 13:24:24.405: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing context model parallel with size 1\n",
      "[2025-09-04 13:24:24.405: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing data parallel with size 1\n",
      "[2025-09-04 13:24:24.405: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing world size to 2\n",
      "[2025-09-04 13:24:24.406: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x77de24b44790>, 'Ascending Ring PG Group')>\n",
      "[2025-09-04 13:24:24.407: I neuronx_distributed/parallel_layers/parallel_state.py:668] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1]]\n",
      "[2025-09-04 13:24:24.407: I neuronx_distributed/parallel_layers/parallel_state.py:669] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1]]\n",
      "[2025-09-04 13:24:24.407: I neuronx_distributed/parallel_layers/parallel_state.py:670] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1]]\n",
      "[2025-09-04 13:24:24.407: I neuronx_distributed/parallel_layers/parallel_state.py:671] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1]]\n",
      "[2025-09-04 13:24:24.407: I neuronx_distributed/parallel_layers/parallel_state.py:672] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1]]\n",
      "[2025-09-04 13:24:24.407: I neuronx_distributed/parallel_layers/parallel_state.py:673] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1]]\n",
      "INFO:Neuron:Generating 1 hlos for key: context_encoding_model\n",
      "INFO:Neuron:Started loading module context_encoding_model\n",
      "INFO:Neuron:Finished loading module context_encoding_model in 0.1381816864013672 seconds\n",
      "INFO:Neuron:generating HLO: context_encoding_model, input example shape = torch.Size([1, 512])\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:485: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/neuron/models/inference/backend/modules/generation/sampling.py:299: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs_soft_max, dim=dim, on_cpu=self.on_cpu)\n",
      "Add predicate {{0,+,-1}<i0=[0:128:1]>,+,0}<i1=[0:2048:1]>\n",
      "start lb and ub of  {0,+,-1}<i0=[0:128:1]> is 0 0\n",
      "Add predicate {{255,+,0}<i0=[0:128:1]>,+,-1}<i1=[0:2048:1]>\n",
      "start lb and ub of  {{255,+,0}<i0=[0:128:1]>,+,-1}<i1=[0:2048:1]> is 255 255\n",
      "before build_invert_ranges alive full {\n",
      "  0 <= i1=[0:2048:1] <= 2047; alive full {\n",
      "    0 <= i1=[0:2048:1] <= 2047; 1 <= i0=[0:128:1] <= 127; alive leaf\n",
      "  }\n",
      "  256 <= i1=[0:2048:1] <= 2047; alive {\n",
      "    256 <= i1=[0:2048:1] <= 2047; 0 <= i0=[0:128:1] <= 127; alive full leaf\n",
      "  }\n",
      "}\n",
      "generated domains alive full {\n",
      "  0 <= i1=[0:2048:1] <= 255; alive {\n",
      "    0 <= i1=[0:2048:1] <= 255; 0 <= i0=[0:128:1] <= 0; alive leaf\n",
      "  }\n",
      "}\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/neuron/models/inference/backend/modules/generation/sampling.py:262: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.on_cpu)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:289: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([1, 512]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.\n",
      "  warnings.warn(\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:289: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.\n",
      "  warnings.warn(\n",
      "INFO:Neuron:Finished generating HLO for context_encoding_model in 2.587005853652954 seconds, input example shape = torch.Size([1, 512])\n",
      "INFO:Neuron:Generating 1 hlos for key: token_generation_model\n",
      "INFO:Neuron:Started loading module token_generation_model\n",
      "INFO:Neuron:Finished loading module token_generation_model in 0.11980652809143066 seconds\n",
      "INFO:Neuron:generating HLO: token_generation_model, input example shape = torch.Size([1, 1])\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:485: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/neuron/models/inference/backend/modules/generation/sampling.py:299: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs_soft_max, dim=dim, on_cpu=self.on_cpu)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/optimum/neuron/models/inference/backend/modules/generation/sampling.py:262: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  probs_cumsum = cumsum(tensor_in=probs, dim=dim, on_cpu=self.on_cpu)\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:289: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.\n",
      "  warnings.warn(\n",
      "INFO:Neuron:Finished generating HLO for token_generation_model in 2.3019745349884033 seconds, input example shape = torch.Size([1, 1])\n",
      "INFO:Neuron:Generated all HLOs in 5.210456848144531 seconds\n",
      "INFO:Neuron:Starting compilation for the priority HLO\n",
      "INFO:Neuron:'token_generation_model' is the priority model with bucket rank 0\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/libneuronxla/neuron_cc_wrapper.py:283: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.\n",
      "  warnings.warn(SyntaxWarning(\n",
      "Fetched cached neuronxcc-2.20.9961.0+0acef03a/MODULE_de733c6f96020a1e5f56+a9d440f5/model.neff from aws-neuron/optimum-neuron-cache\n",
      "2025-09-04 13:24:30.000210:  255113  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_de733c6f96020a1e5f56+a9d440f5/model.neff\n",
      "Fetched cached neuronxcc-2.20.9961.0+0acef03a/MODULE_de733c6f96020a1e5f56+a9d440f5/wrapped_neff.hlo from aws-neuron/optimum-neuron-cache\n",
      "INFO:Neuron:Done compilation for the priority HLO in 0.8264033794403076 seconds\n",
      "INFO:Neuron:Updating the hlo module with optimized layout\n",
      "INFO:Neuron:Done optimizing weight layout for all HLOs in 0.5720491409301758 seconds\n",
      "INFO:Neuron:Starting compilation for all HLOs\n",
      "INFO:Neuron:Neuron compiler flags: --auto-cast=none --model-type=transformer --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma ' -O2  --lnc=1 --logfile=/tmp/nxd_model/context_encoding_model/_tp0_bk0/log-neuron-cc.txt\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/libneuronxla/neuron_cc_wrapper.py:245: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.\n",
      "  warnings.warn(SyntaxWarning(\n",
      "Fetched cached neuronxcc-2.20.9961.0+0acef03a/MODULE_057bc784fc164fb34d3e+ed72d204/model.neff from aws-neuron/optimum-neuron-cache\n",
      "2025-09-04 13:24:31.000549:  255113  INFO ||NEURON_CC_WRAPPER||: Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_057bc784fc164fb34d3e+ed72d204/model.neff\n",
      "INFO:Neuron:Finished Compilation for all HLOs in 0.5351839065551758 seconds\n",
      "Fetched cached neuronxcc-2.20.9961.0+0acef03a/MODULE_e3031c3c114f7c905db7+ae6a382b/model.neff from aws-neuron/optimum-neuron-cache\n",
      "neuronxcc-2.20.9961.0+0acef03a/MODULE_e3031c3c114f7c905db7+ae6a382b/wrapped_neff.hlo not found in aws-neuron/optimum-neuron-cache: the corresponding graph will be recompiled. This may take up to one hour for large models.\n",
      "INFO:Neuron:Using a cached neff at /var/tmp/neuron-compile-cache/neuronxcc-2.20.9961.0+0acef03a/MODULE_e3031c3c114f7c905db7+ae6a382b/model.neff\n",
      "INFO:Neuron:Done preparing weight layout transformation\n",
      "INFO:Neuron:Finished building model in 7.8153510093688965 seconds\n",
      "Configuration saved in /home/ubuntu/environment/ml/qwen/compiled_model/neuron_config.json\n",
      "INFO:Neuron:Sharding Weights for ranks: 0...1\n",
      "[2025-09-04 13:24:32.363: I neuronx_distributed/parallel_layers/parallel_state.py:628] > initializing tensor model parallel with size 2\n",
      "[2025-09-04 13:24:32.364: I neuronx_distributed/parallel_layers/parallel_state.py:629] > initializing pipeline model parallel with size 1\n",
      "[2025-09-04 13:24:32.364: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing context model parallel with size 1\n",
      "[2025-09-04 13:24:32.364: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing data parallel with size 1\n",
      "[2025-09-04 13:24:32.364: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing world size to 2\n",
      "[2025-09-04 13:24:32.365: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x77de24b44790>, 'Ascending Ring PG Group')>\n",
      "[2025-09-04 13:24:32.365: I neuronx_distributed/parallel_layers/parallel_state.py:668] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1]]\n",
      "[2025-09-04 13:24:32.365: I neuronx_distributed/parallel_layers/parallel_state.py:669] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1]]\n",
      "[2025-09-04 13:24:32.365: I neuronx_distributed/parallel_layers/parallel_state.py:670] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1]]\n",
      "[2025-09-04 13:24:32.365: I neuronx_distributed/parallel_layers/parallel_state.py:671] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1]]\n",
      "[2025-09-04 13:24:32.365: I neuronx_distributed/parallel_layers/parallel_state.py:672] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1]]\n",
      "[2025-09-04 13:24:32.366: I neuronx_distributed/parallel_layers/parallel_state.py:673] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1]]\n",
      "/opt/aws_neuronx_venv_pytorch_latest/lib/python3.10/site-packages/neuronx_distributed/trace/trace.py:640: UserWarning: Removing redundant keys from checkpoint: []\n",
      "  warnings.warn(f\"Removing redundant keys from checkpoint: {keys_to_delete}\")\n",
      "INFO:Neuron:Done Sharding weights in 33.1569037700101\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export neuron --model /home/ubuntu/environment/ml/qwen/merged_model --task text-generation --sequence_length 512 --batch_size 1 --num_cores 2 /home/ubuntu/environment/ml/qwen/compiled_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "We will install the Optimum Neuron vllm option.  Then, run inference using the compiled model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install optimum-neuron[vllm]==0.3.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-04 13:30:37 [config.py:841] This model supports multiple tasks: {'classify', 'embed', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 09-04 13:30:37 [config.py:1472] Using max model len 2048\n",
      "INFO 09-04 13:30:37 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2) with config: model='/home/ubuntu/environment/ml/qwen/compiled_model', speculative_config=None, tokenizer='/home/ubuntu/environment/ml/qwen/compiled_model', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cpu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/home/ubuntu/environment/ml/qwen/compiled_model, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":1,\"local_cache_dir\":null}, use_cached_outputs=False, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Neuron:Loading sharded checkpoint from /home/ubuntu/environment/ml/qwen/compiled_model/checkpoint/weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 09-04 13:30:39 [config.py:4861] Current vLLM config is not set.\n",
      "INFO 09-04 13:30:39 [executor_base.py:113] # neuron blocks: 2, # CPU blocks: 0\n",
      "INFO 09-04 13:30:39 [executor_base.py:118] Maximum concurrency for 2048 tokens per request: 2.00x\n",
      "INFO 09-04 13:30:39 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 0.00 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453fc3eae52742329f88dbc2afbfee14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b15fe6455a942d497e089fe61a4a499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################################################\n",
      "Prompt: '\\n<|system|>\\nYou are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE management (department_id VARCHAR); CREATE TABLE department (department_id VARCHAR)</s>\\n<|user|>\\nHow many departments are led by heads who are not mentioned?</s>\\n<|assistant|>\\n', \n",
      "\n",
      " Generated text: 'SELECT COUNT(*) FROM management WHERE department_id NOT IN (SELECT department_id FROM department);' \n",
      "\n",
      "Prompt: '\\n<|system|>\\nYou are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE courses (course_name VARCHAR, course_id VARCHAR); CREATE TABLE \\nstudent_course_registrations (student_id VARCHAR, course_id VARCHAR)</s>\\n<|user|>\\nWhat are the ids of all students for courses and what are the names of those courses?</s>\\n<|assistant|>\\n', \n",
      "\n",
      " Generated text: 'SELECT T2.course_id, T2.course_name FROM student_course_registrations AS T1 JOIN courses AS T2 ON T1.course_id = T2.course_id;' \n",
      "\n",
      "Prompt: '\\n<|system|>\\nYou are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\\nSCHEMA:\\nCREATE TABLE table_name_9 (wins INTEGER, year VARCHAR, team VARCHAR, points VARCHAR)</s>\\n<|user|>\\nWhich highest wins number had Kawasaki as a team, 95 points, and a year prior to 1981?</s>\\n<|assistant|>\\n', \n",
      "\n",
      " Generated text: '<|start_of_turn|>\\n<s>\\nSELECT MAX(wins) FROM table_name_9 WHERE team = \"kawasaki\" AND points = 95 AND year < 1981;' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "llm = LLM(\n",
    "    model=\"/home/ubuntu/environment/ml/qwen/compiled_model\", #local compiled model\n",
    "    max_num_seqs=1,\n",
    "    max_model_len=2048,\n",
    "    device=\"neuron\",\n",
    "    tensor_parallel_size=2,\n",
    "    override_neuron_config={})\n",
    "example1=\"\"\"\n",
    "<|im_start|>system\n",
    "You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "CREATE TABLE management (department_id VARCHAR); CREATE TABLE department (department_id VARCHAR)<|im_end|>\n",
    "<|im_start|>user\n",
    "How many departments are led by heads who are not mentioned?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "example2=\"\"\"\n",
    "<|im_start|>system\n",
    "You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "CREATE TABLE courses (course_name VARCHAR, course_id VARCHAR); CREATE TABLE student_course_registrations (student_id VARCHAR, course_id VARCHAR)<|im_end|>\n",
    "<|im_start|>user\n",
    "What are the ids of all students for courses and what are the names of those courses?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "example3=\"\"\"\n",
    "<|im_start|>system\n",
    "You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "CREATE TABLE table_name_9 (wins INTEGER, year VARCHAR, team VARCHAR, points VARCHAR)<|im_end|>\n",
    "<|im_start|>user\n",
    "Which highest wins number had Kawasaki as a team, 95 points, and a year prior to 1981?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "prompts = [\n",
    "    example1,\n",
    "    example2,\n",
    "    example3\n",
    "]\n",
    "\n",
    "sampling_params = SamplingParams(max_tokens=2048, temperature=0.8)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "print(\"#########################################################\")\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, \\n\\n Generated text: {generated_text!r} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuronx_venv_pytorch_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
