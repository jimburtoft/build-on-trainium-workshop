{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "In this notebook, we showcase how to fine-tune the Qwen3-1.7B model on AWS Trainium using the Hugging Face Optimum Neuron library.\n",
    "The goal of this task is Text-to-SQL generation — training the model to translate natural language questions into executable SQL queries.\n",
    "\n",
    "We will fine-tune the model using `optimum.neuron`, save the trained checkpoint, and then deploy it for inference with Optimum-Neuron[vllm], enabling high-performance, low-latency Text-to-SQL execution.\n",
    "\n",
    "By the end of this notebook, you’ll have a fine-tuned, Trainium-optimized Qwen3 model ready for deployment and real-time inference. This workflow demonstrates how to leverage the Optimum Neuron toolchain to efficiently train and serve large language models on AWS Neuron devices.\n",
    "\n",
    "For this module, you will be using the [b-mc2/sql-create-context](https://huggingface.co/datasets/b-mc2/sql-create-context) dataset which consists of thousands of examples of SQL schemas, questions about the schemas, and SQL queries intended to answer the questions.\n",
    "\n",
    "*Dataset example 1:*\n",
    "* *SQL schema/context:* `CREATE TABLE management (department_id VARCHAR); CREATE TABLE department (department_id VARCHAR)`\n",
    "* *Question:* `How many departments are led by heads who are not mentioned?`\n",
    "* *SQL query/answer:* `SELECT COUNT(*) FROM department WHERE NOT department_id IN (SELECT department_id FROM management)`\n",
    "\n",
    "*Dataset example 2:*\n",
    "* *SQL schema/context:* `CREATE TABLE courses (course_name VARCHAR, course_id VARCHAR); CREATE TABLE student_course_registrations (student_id VARCHAR, course_id VARCHAR)`\n",
    "* *Question:* `What are the ids of all students for courses and what are the names of those courses?`\n",
    "* *SQL query/answer:* `SELECT T1.student_id, T2.course_name FROM student_course_registrations AS T1 JOIN courses AS T2 ON T1.course_id = T2.course_id`\n",
    "\n",
    "By fine-tuning the model over several thousand of these text-to-SQL examples, the model will then learn how to generate an appropriate SQL query when presented with a SQL context and a free-form question.\n",
    "\n",
    "This text-to-SQL use case was selected so you can successfully fine-tune your model in a reasonably short amount of time (~25 minutes) which is appropriate for this workshop. Although this is a relatively simple use case, please keep in mind that the same techniques and components used in this module can also be applied to fine-tune LLMs for more advanced use cases such as writing code, summarizing documents, creating blog posts - the possibilities are endless!\n",
    "\n",
    "# Install requirements\n",
    "This notebook uses [Hugging Face Optimum Neuron](https://github.com/huggingface/optimum-neuron) which works like an interface between the Hugging Face Transformers library and AWS Accelerators including AWS Trainium and AWS Inferentia. We will also install some other libraries like peft, trl etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ubuntu/neuron-workshops/labs/FineTuning/HuggingFaceExample/01_finetuning/assets\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning\n",
    "\n",
    "In this section, we fine-tune the Qwen3-1.7B model on the Text-to-SQL task using Hugging Face Optimum Neuron. Here are the parameters we are going to pass - \n",
    "\n",
    "1. `--nnodes`:\tNumber of nodes (1 = single node)\n",
    "2. `--nproc_per_node`: \tProcesses per node (usually equals number of devices).\n",
    "3. `--model_id, --tokenizer_id`:\tModel and tokenizer identifiers (from Hugging Face or local path).\n",
    "4. `--output_dir`:\tDirectory for saving checkpoints and logs.\n",
    "5. `--bf16`:\tEnables bfloat16 precision for faster, memory-efficient training.\n",
    "5. `--gradient_checkpointing`:\tSaves memory by recomputing activations during backprop.\n",
    "6. `--gradient_accumulation_steps`:\tSteps to accumulate gradients before optimizer update.\n",
    "7. `--learning_rate`:\tInitial training learning rate.\n",
    "8. `--max_steps`:\tTotal number of training steps.\n",
    "9. `--per_device_train_batch_size`:\tBatch size per device.\n",
    "10. `--tensor_parallel_size`:\tNumber of devices for tensor parallelism.\n",
    "11. `--lora_r, --lora_alpha, --lora_dropout`:\tLoRA hyperparameters — rank, scaling, and dropout rate.\n",
    "12. `--dataloader_drop_last`:\tDrops last incomplete batch.\n",
    "13. `--disable_tqdm`: Disables progress bar.\n",
    "14. `--logging_steps`:\tLog interval (in steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun \\\n",
    "  --nnodes 1 \\\n",
    "  --nproc_per_node 2 \\\n",
    "  finetune_model.py \\\n",
    "  --model_id Qwen/Qwen3-1.7B \\\n",
    "  --tokenizer_id Qwen/Qwen3-1.7B \\\n",
    "  --output_dir ~/environment/ml/qwen \\\n",
    "  --bf16 True \\\n",
    "  --gradient_checkpointing True \\\n",
    "  --gradient_accumulation_steps 1 \\\n",
    "  --learning_rate 5e-5 \\\n",
    "  --max_steps 1000 \\\n",
    "  --per_device_train_batch_size 2 \\\n",
    "  --tensor_parallel_size 2 \\\n",
    "  --lora_r 16 \\\n",
    "  --lora_alpha 32 \\\n",
    "  --lora_dropout 0.05 \\\n",
    "  --dataloader_drop_last True \\\n",
    "  --disable_tqdm True \\\n",
    "  --logging_steps 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilation\n",
    "\n",
    "After completing the fine-tuning process, the next step is to compile the trained model for AWS Trainium inference using the Hugging Face Optimum Neuron toolchain.\n",
    "Neuron compilation optimizes the model graph and converts it into a Neuron Executable File Format (NEFF), enabling efficient execution on NeuronCores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!optimum-cli export neuron \\\n",
    "  --model /home/ubuntu/neuron-workshops/labs/ml/qwen/merged_model \\\n",
    "  --task text-generation \\\n",
    "  --sequence_length 512 \\\n",
    "  --batch_size 1 \\\n",
    "  /home/ubuntu/neuron-workshops/labs/ml/qwen/compiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "We will install the Optimum Neuron vllm library.  Then, run inference using the compiled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install optimum-neuron[vllm]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "llm = LLM(\n",
    "    model=\"/home/ubuntu/neuron-workshops/labs/ml/qwen/compiled_model\", #local compiled model\n",
    "    max_num_seqs=1,\n",
    "    max_model_len=2048,\n",
    "    device=\"neuron\",\n",
    "    tensor_parallel_size=2,\n",
    "    override_neuron_config={})\n",
    "example1=\"\"\"\n",
    "<|im_start|>system\n",
    "You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "CREATE TABLE management (department_id VARCHAR); CREATE TABLE department (department_id VARCHAR)<|im_end|>\n",
    "<|im_start|>user\n",
    "How many departments are led by heads who are not mentioned?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "example2=\"\"\"\n",
    "<|im_start|>system\n",
    "You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "CREATE TABLE courses (course_name VARCHAR, course_id VARCHAR); CREATE TABLE student_course_registrations (student_id VARCHAR, course_id VARCHAR)<|im_end|>\n",
    "<|im_start|>user\n",
    "What are the ids of all students for courses and what are the names of those courses?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "example3=\"\"\"\n",
    "<|im_start|>system\n",
    "You are a text to SQL query translator. Users will ask you questions in English and you will generate a SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "CREATE TABLE table_name_9 (wins INTEGER, year VARCHAR, team VARCHAR, points VARCHAR)<|im_end|>\n",
    "<|im_start|>user\n",
    "Which highest wins number had Kawasaki as a team, 95 points, and a year prior to 1981?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "prompts = [\n",
    "    example1,\n",
    "    example2,\n",
    "    example3\n",
    "]\n",
    "\n",
    "sampling_params = SamplingParams(max_tokens=2048, temperature=0.8)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "print(\"#########################################################\")\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, \\n\\n Generated text: {generated_text!r} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuronx_venv_pytorch_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
