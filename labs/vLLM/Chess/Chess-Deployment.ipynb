{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chess Model Deployment with vLLM on Trainium\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates deploying a fine-tuned chess model using vLLM on AWS Trainium. You'll learn how to:\n",
    "\n",
    "- Set up vLLM with Neuron backend for Trainium\n",
    "- Deploy a compiled chess model with batch support\n",
    "- Test the deployed model with interactive games\n",
    "- Validate performance and understand concurrency benefits\n",
    "\n",
    "**Prerequisites:**\n",
    "- Complete the fine-tuning lab or use a pre-trained chess model\n",
    "- Trainium instance (trn1.2xlarge or larger)\n",
    "- Neuron SDK 2.20+ (pre-installed in Workshop Studio)\n",
    "\n",
    "**Duration:** 30-40 minutes (including model compilation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "First, let's install the chess environment and its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ubuntu/environment/neuron-workshops/labs/vLLM/Chess/assets\n",
    "\n",
    "# Install chess environment dependencies\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "print(\"\\n✅ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Verify Neuron SDK Environment\n",
    "\n",
    "Let's check that your Neuron SDK installation has the required versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def get_version(package):\n",
    "    try:\n",
    "        result = subprocess.run([sys.executable, \"-m\", \"pip\", \"show\", package], \n",
    "                              capture_output=True, text=True, check=True)\n",
    "        for line in result.stdout.split('\\n'):\n",
    "            if line.startswith('Version:'):\n",
    "                return line.split(':')[1].strip()\n",
    "    except:\n",
    "        return \"Not installed\"\n",
    "\n",
    "print(\"Neuron SDK Environment:\")\n",
    "print(f\"  neuronx-cc: {get_version('neuronx-cc')}\")\n",
    "print(f\"  torch-neuronx: {get_version('torch-neuronx')}\")\n",
    "print(f\"  optimum-neuron: {get_version('optimum-neuron')}\")\n",
    "print(f\"  torch: {get_version('torch')}\")\n",
    "\n",
    "# Check minimum versions\n",
    "min_versions = {\n",
    "    'neuronx-cc': '2.15',\n",
    "    'torch-neuronx': '2.1', \n",
    "    'optimum-neuron': '0.3.0'\n",
    "}\n",
    "\n",
    "print(\"\\nVersion Check:\")\n",
    "all_good = True\n",
    "for pkg, min_ver in min_versions.items():\n",
    "    current = get_version(pkg)\n",
    "    if current != \"Not installed\" and current >= min_ver:\n",
    "        print(f\"✓ {pkg}: {current} (minimum: {min_ver})\")\n",
    "    else:\n",
    "        print(f\"⚠ {pkg}: {current} (minimum required: {min_ver})\")\n",
    "        all_good = False\n",
    "\n",
    "if all_good:\n",
    "    print(\"\\n✅ Environment ready for chess deployment!\")\n",
    "else:\n",
    "    print(\"\\n⚠️  Some packages may need updating. See README troubleshooting section.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Check Neuron Hardware\n",
    "\n",
    "Verify Neuron cores are available and not in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!neuron-ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:** You should see 2 Neuron cores available for trn1.2xlarge.\n",
    "\n",
    "If cores are in use, you can free them with: `!pkill -f vllm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configure Model Path\n",
    "\n",
    "Set the path to your fine-tuned chess model. If you completed the fine-tuning lab, update the path below.\n",
    "\n",
    "Otherwise, we'll use a pre-compiled model from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Option 1: Use your fine-tuned model (update path if you completed fine-tuning lab)\n",
    "# MODEL_PATH = \"/home/ubuntu/environment/ChessLM_Qwen3_Trainium\"\n",
    "\n",
    "# Option 2: Use pre-compiled model from repository (default)\n",
    "MODEL_PATH = \"/home/ubuntu/ChessLM_Qwen3_Trainium\"\n",
    "\n",
    "# Verify model exists\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(f\"✅ Model found at: {MODEL_PATH}\")\n",
    "    \n",
    "    # Check if model is compiled\n",
    "    if os.path.exists(os.path.join(MODEL_PATH, \"model.pt\")):\n",
    "        print(\"✅ Pre-compiled model found (model.pt)\")\n",
    "        \n",
    "        # Check neuron_config\n",
    "        import json\n",
    "        with open(os.path.join(MODEL_PATH, \"neuron_config.json\")) as f:\n",
    "            config = json.load(f)\n",
    "        print(f\"   Batch size: {config.get('batch_size', 'N/A')}\")\n",
    "        print(f\"   Continuous batching: {config.get('continuous_batching', 'N/A')}\")\n",
    "        print(f\"   Tensor parallel: {config.get('tp_degree', 'N/A')}\")\n",
    "    else:\n",
    "        print(\"⚠️  Model not compiled yet. Will compile on first run (10-30 min).\")\n",
    "else:\n",
    "    print(f\"❌ Model not found at: {MODEL_PATH}\")\n",
    "    print(\"   Please update MODEL_PATH or complete the fine-tuning lab first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Start vLLM Server\n",
    "\n",
    "Now let's start the vLLM server with Neuron backend. The server is configured for:\n",
    "- **batch_size=4**: Can process up to 4 requests concurrently\n",
    "- **continuous_batching=true**: Automatically groups concurrent requests\n",
    "- **tensor_parallel_size=2**: Uses both Neuron cores on trn1.2xlarge\n",
    "\n",
    "**Note:** First-time startup includes model compilation which takes 10-30 minutes. Subsequent starts are fast (<1 min)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update vllm.sh with correct model path\n",
    "import os\n",
    "\n",
    "vllm_script = \"vllm-server/vllm.sh\"\n",
    "with open(vllm_script, 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Update MODEL_PATH in script\n",
    "content = content.replace(\n",
    "    'MODEL_PATH=\"/home/ubuntu/ChessLM_Qwen3_Trainium\"',\n",
    "    f'MODEL_PATH=\"{MODEL_PATH}\"'\n",
    ")\n",
    "\n",
    "with open(vllm_script, 'w') as f:\n",
    "    f.write(content)\n",
    "\n",
    "print(f\"✅ Updated vllm.sh with model path: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start vLLM server in background\n",
    "!cd vllm-server && bash vllm.sh\n",
    "\n",
    "print(\"\\nvLLM server starting...\")\n",
    "print(\"This may take 10-30 minutes on first run (model compilation).\")\n",
    "print(\"Subsequent runs will be fast (<1 minute).\")\n",
    "print(\"\\nWait for 'Application startup complete' message before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**While waiting for server startup:**\n",
    "\n",
    "You can monitor the server logs in the terminal:\n",
    "```bash\n",
    "tail -f /home/ubuntu/environment/neuron-workshops/labs/vLLM/Chess/assets/vllm-server/vllm-server.log\n",
    "```\n",
    "\n",
    "Or check Neuron core usage:\n",
    "```bash\n",
    "watch -n 2 neuron-ls\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Server Connection\n",
    "\n",
    "Once the server has started, let's test the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import openai\n",
    "\n",
    "# Give server a moment to finish startup\n",
    "print(\"Waiting for server to be fully ready...\")\n",
    "time.sleep(10)\n",
    "\n",
    "# Initialize OpenAI client pointing to vLLM server\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"not-needed\"  # vLLM doesn't require authentication\n",
    ")\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_PATH,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "        max_tokens=10,\n",
    "        extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}}\n",
    "    )\n",
    "    print(\"✅ vLLM server is running!\")\n",
    "    print(f\"\\nTest response: {response.choices[0].message.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Connection failed: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Check if server is still starting (check logs)\")\n",
    "    print(\"2. Verify Neuron cores are not in use: neuron-ls\")\n",
    "    print(\"3. Check for errors: tail vllm-server/vllm-server.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Play Your First Chess Game\n",
    "\n",
    "Now that the server is running, let's play a quick game against a random opponent to verify everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import chess environment\n",
    "import sys\n",
    "sys.path.insert(0, '/home/ubuntu/environment/neuron-workshops/labs/vLLM/Chess/assets')\n",
    "\n",
    "from env import ChessEnvironment\n",
    "from agents import VLLMAgent, RandomAgent\n",
    "\n",
    "# Create agents\n",
    "vllm_agent = VLLMAgent(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    model=MODEL_PATH,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "random_agent = RandomAgent()\n",
    "\n",
    "# Create environment\n",
    "env = ChessEnvironment(\n",
    "    white_agent=vllm_agent,\n",
    "    black_agent=random_agent,\n",
    "    max_moves=100\n",
    ")\n",
    "\n",
    "# Play game\n",
    "print(\"Playing chess game: vLLM vs Random\\n\")\n",
    "result = env.play_game(verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Game Result\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Result: {result['result']}\")\n",
    "print(f\"Moves played: {result['moves_played']}\")\n",
    "print(f\"Reason: {result['game_over_reason']}\")\n",
    "print(f\"\\nWhite (vLLM) illegal attempts: {result['white_illegal_attempts']}\")\n",
    "print(f\"Black (Random) illegal attempts: {result['black_illegal_attempts']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test Against Stockfish\n",
    "\n",
    "Now let's test against a real chess engine to get a sense of the model's strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import StockfishAgent\n",
    "\n",
    "# Create Stockfish opponent (skill level 1 = beginner)\n",
    "stockfish = StockfishAgent(skill_level=1, depth=2)\n",
    "\n",
    "# Create environment\n",
    "env = ChessEnvironment(\n",
    "    white_agent=vllm_agent,\n",
    "    black_agent=stockfish,\n",
    "    max_moves=100\n",
    ")\n",
    "\n",
    "# Play game\n",
    "print(\"Playing chess game: vLLM vs Stockfish (skill=1)\\n\")\n",
    "result = env.play_game(verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Game Result\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Result: {result['result']}\")\n",
    "print(f\"Moves played: {result['moves_played']}\")\n",
    "print(f\"Reason: {result['game_over_reason']}\")\n",
    "print(f\"\\nYour model's illegal move attempts: {result['white_illegal_attempts']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Validate Performance\n",
    "\n",
    "Let's measure single-move latency to understand performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import time\n",
    "\n",
    "# Create a fresh board\n",
    "board = chess.Board()\n",
    "legal_moves = list(board.legal_moves)\n",
    "\n",
    "# Measure latency over 10 moves\n",
    "latencies = []\n",
    "print(\"Measuring latency over 10 moves...\\n\")\n",
    "\n",
    "for i in range(10):\n",
    "    start = time.time()\n",
    "    move, comment = vllm_agent.choose_move(\n",
    "        board=board.copy(),\n",
    "        legal_moves=legal_moves,\n",
    "        move_history=[],\n",
    "        side_to_move=\"White\"\n",
    "    )\n",
    "    latency = time.time() - start\n",
    "    latencies.append(latency)\n",
    "    print(f\"Move {i+1}: {latency:.3f}s\")\n",
    "\n",
    "# Calculate statistics\n",
    "import statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Performance Metrics\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean latency: {statistics.mean(latencies):.3f}s\")\n",
    "print(f\"Median latency: {statistics.median(latencies):.3f}s\")\n",
    "print(f\"Min latency: {min(latencies):.3f}s\")\n",
    "print(f\"Max latency: {max(latencies):.3f}s\")\n",
    "print(f\"Throughput: {1/statistics.mean(latencies):.2f} moves/second\")\n",
    "\n",
    "# Expected performance\n",
    "expected_latency = 0.65  # seconds\n",
    "if statistics.mean(latencies) < expected_latency * 1.2:\n",
    "    print(\"\\n✅ Performance looks good!\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  Latency higher than expected (~{expected_latency}s). Check server configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Concurrency Benefits\n",
    "\n",
    "Your vLLM server is configured with:\n",
    "- **`max_num_seqs=4`**: Can batch up to 4 requests\n",
    "- **`continuous_batching=true`**: Automatically groups concurrent requests\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "When you run multiple games in parallel (covered in Chess-Tournament.ipynb), the tournament scheduler uses Python multiprocessing to run games concurrently. Each game makes HTTP requests to the same vLLM server.\n",
    "\n",
    "**With 1 game (sequential):**\n",
    "- Latency: ~0.65s per move\n",
    "- Throughput: 1.54 moves/second\n",
    "\n",
    "**With 4 games (parallel):**\n",
    "- Latency per game: ~1.86s per move (games wait in small batches)\n",
    "- Total throughput: 2.15 moves/second across all games\n",
    "- **Improvement: 1.4x faster** (games complete ~40% faster overall)\n",
    "\n",
    "**Why the improvement?**\n",
    "The vLLM server batches concurrent requests together, processing them simultaneously on the Neuron cores. This amortizes the overhead of model inference across multiple requests.\n",
    "\n",
    "### Testing Concurrency:\n",
    "\n",
    "You'll see this in action in the next notebook (Chess-Tournament.ipynb), where running with `--parallelism 4` enables automatic batching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "\n",
    "✅ Deployed a chess model via vLLM on Trainium  \n",
    "✅ Configured vLLM for batching (max_num_seqs=4, continuous_batching)  \n",
    "✅ Tested the model with interactive games  \n",
    "✅ Validated performance metrics  \n",
    "✅ Understood concurrency benefits (1.4x throughput improvement)\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "Continue to [Chess-Tournament.ipynb](Chess-Tournament.ipynb) to:\n",
    "- Run competitive tournaments with TrueSkill ratings\n",
    "- Evaluate model strength against various Stockfish levels\n",
    "- See automatic batching in action with parallel games\n",
    "- Analyze detailed performance metrics\n",
    "\n",
    "**Keep the vLLM server running** for the next notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
