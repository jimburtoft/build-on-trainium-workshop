{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chess Model Deployment with vLLM on Trainium\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates deploying a fine-tuned chess model using vLLM on AWS Trainium. You'll learn how to:\n",
    "\n",
    "- Set up vLLM with Neuron backend for Trainium\n",
    "- Deploy a compiled chess model with batch support\n",
    "- Test the deployed model with interactive games\n",
    "- Validate performance and understand concurrency benefits\n",
    "\n",
    "**Prerequisites:**\n",
    "- Complete the fine-tuning lab or use a pre-trained chess model\n",
    "- Trainium instance (trn1.2xlarge or trn2.3xlarge)\n",
    "\n",
    "**Duration:** 30-40 minutes (including model compilation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "First, let's install the chess environment and its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/environment/neuron-workshops/labs/vLLM/Chess/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "✅ Dependencies installed successfully!\n"
     ]
    }
   ],
   "source": [
    "%cd /home/ubuntu/environment/neuron-workshops/labs/vLLM/Chess/assets\n",
    "\n",
    "# Install chess environment dependencies\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "print(\"\\n Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Check Neuron Hardware\n",
    "\n",
    "Verify Neuron cores are available and not in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance-type: trn1.2xlarge\n",
      "instance-id: i-039d7490fb75ae0e2\n",
      "+--------+--------+----------+--------+--------------+----------+------+\n",
      "| NEURON | NEURON |  NEURON  | NEURON |     PCI      |   CPU    | NUMA |\n",
      "| DEVICE | CORES  | CORE IDS | MEMORY |     BDF      | AFFINITY | NODE |\n",
      "+--------+--------+----------+--------+--------------+----------+------+\n",
      "| 0      | 2      | 0-1      | 32 GB  | 0000:00:1e.0 | 0-7      | -1   |\n",
      "+--------+--------+----------+--------+--------------+----------+------+\n"
     ]
    }
   ],
   "source": [
    "!neuron-ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting neuronx-cc==2.21.*\n",
      "  Downloading https://pip.repos.neuron.amazonaws.com/neuronx-cc/neuronx_cc-2.21.33363.0%2B82129205-cp310-cp310-linux_x86_64.whl (639.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m639.6/639.6 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx~=2.6 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from neuronx-cc==2.21.*) (2.8.8)\n",
      "Requirement already satisfied: scipy<=1.12.0,>=1.10.1 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from neuronx-cc==2.21.*) (1.12.0)\n",
      "Requirement already satisfied: python-daemon>=2.2.4 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from neuronx-cc==2.21.*) (3.1.2)\n",
      "Requirement already satisfied: requests-unixsocket>=0.1.5 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from neuronx-cc==2.21.*) (0.4.1)\n",
      "Requirement already satisfied: psutil>=5.6.7 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from neuronx-cc==2.21.*) (7.0.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.3 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from neuronx-cc==2.21.*) (1.25.2)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from neuronx-cc==2.21.*) (0.5.3)\n",
      "Requirement already satisfied: islpy~=2023.1 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from neuronx-cc==2.21.*) (2023.2.5)\n",
      "Requirement already satisfied: protobuf>=3 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from neuronx-cc==2.21.*) (3.20.3)\n",
      "Requirement already satisfied: pgzip>=0.3.0 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from neuronx-cc==2.21.*) (0.3.5)\n",
      "Requirement already satisfied: ec2-metadata~=2.10 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from neuronx-cc==2.21.*) (2.14.0)\n",
      "Requirement already satisfied: tqdm>=4 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from neuronx-cc==2.21.*) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from neuronx-cc==2.21.*) (4.14.1)\n",
      "Requirement already satisfied: requests in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from ec2-metadata~=2.10->neuronx-cc==2.21.*) (2.32.4)\n",
      "Requirement already satisfied: lockfile>=0.10 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from python-daemon>=2.2.4->neuronx-cc==2.21.*) (0.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from requests->ec2-metadata~=2.10->neuronx-cc==2.21.*) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from requests->ec2-metadata~=2.10->neuronx-cc==2.21.*) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from requests->ec2-metadata~=2.10->neuronx-cc==2.21.*) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages (from requests->ec2-metadata~=2.10->neuronx-cc==2.21.*) (2025.7.14)\n",
      "Installing collected packages: neuronx-cc\n",
      "  Attempting uninstall: neuronx-cc\n",
      "    Found existing installation: neuronx-cc 2.20.9961.0+0acef03a\n",
      "    Uninstalling neuronx-cc-2.20.9961.0+0acef03a:\n",
      "      Successfully uninstalled neuronx-cc-2.20.9961.0+0acef03a\n",
      "Successfully installed neuronx-cc-2.21.33363.0+82129205\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "NeuronX Compiler version 2.21.33363.0+82129205\n",
      "\n",
      "Python version 3.10.12\n",
      "HWM version 2.21.0.33363+82129205\n",
      "NumPy version 1.25.2\n",
      "\n",
      "Running on AMI ami-040348201d80b58ad\n",
      "Running in region usw2-az4\n"
     ]
    }
   ],
   "source": [
    "\n",
    " # Upgrade to neuronx-cc 2.21.x\n",
    " !pip install --upgrade neuronx-cc==2.21.* --extra-index-url=https://pip.repos.neuron.amazonaws.com\n",
    " # Verify\n",
    " !neuronx-cc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:** You should see 2 Neuron cores available for trn1.2xlarge or 4 Neuron cores available for trn2.3xlarge\n",
    "If cores are in use, you can free them with: `!pkill -f vllm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: neuronx-cc\n",
      "Version: 2.20.9961.0+0acef03a\n",
      "Summary: AWS Neuron SDK compiler\n",
      "Home-page: https://aws.amazon.com/machine-learning/inferentia/\n",
      "Author: \n",
      "Author-email: \n",
      "License: AWS Neuron Software License Agreement\n",
      "Location: /opt/aws_neuronx_venv_pytorch_2_7_nxd_inference/lib/python3.10/site-packages\n",
      "Requires: ec2-metadata, islpy, ml-dtypes, networkx, numpy, pgzip, protobuf, psutil, python-daemon, requests-unixsocket, scipy, tqdm, typing-extensions\n",
      "Required-by: libneuronxla\n"
     ]
    }
   ],
   "source": [
    "!pip show neuronx-cc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configure Model Path\n",
    "\n",
    "Set the path to your fine-tuned chess model. If you completed the fine-tuning lab, update the path below.\n",
    "\n",
    "Otherwise, we'll use a pre-compiled model from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model not found at: kunhunjon/ChessLM_Qwen3_Trainium_AWS_Format\n",
      "Please update MODEL_PATH or complete the fine-tuning lab first.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Option 1: Use your fine-tuned model (update path if you completed fine-tuning lab)\n",
    "# MODEL_PATH = \"/home/ubuntu/environment/ChessLM_Qwen3_Trainium\"\n",
    "\n",
    "# Option 2: Use pre-compiled model from repository (default)\n",
    "MODEL_PATH = \"kunhunjon/ChessLM_Qwen3_Trainium_AWS_Format\"\n",
    "\n",
    "# Verify model exists\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(f\" Model found at: {MODEL_PATH}\")\n",
    "    \n",
    "    # Check if model is compiled\n",
    "    if os.path.exists(os.path.join(MODEL_PATH, \"model.pt\")):\n",
    "        print(\" Pre-compiled model found (model.pt)\")\n",
    "        \n",
    "        # Check neuron_config\n",
    "        import json\n",
    "        with open(os.path.join(MODEL_PATH, \"neuron_config.json\")) as f:\n",
    "            config = json.load(f)\n",
    "        print(f\"   Batch size: {config.get('batch_size', 'N/A')}\")\n",
    "        print(f\"   Continuous batching: {config.get('continuous_batching', 'N/A')}\")\n",
    "        print(f\"   Tensor parallel: {config.get('tp_degree', 'N/A')}\")\n",
    "    else:\n",
    "        print(\"Model not compiled yet. Will compile on first run (10-30 min).\")\n",
    "else:\n",
    "    print(f\" Model not found at: {MODEL_PATH}\")\n",
    "    print(\"Please update MODEL_PATH or complete the fine-tuning lab first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Start vLLM Server\n",
    "\n",
    "Now let's start the vLLM server with Neuron backend. The server is configured for:\n",
    "- **batch_size=4**: Can process up to 4 requests concurrently\n",
    "- **continuous_batching=true**: Automatically groups concurrent requests\n",
    "- **tensor_parallel_size=2**: \n",
    "\n",
    "**Note:** First-time startup includes model compilation which takes 10-30 minutes. Subsequent starts are fast (<1 min)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Updated vllm.sh with model path: /home/ubuntu/chess-model-qwen\n"
     ]
    }
   ],
   "source": [
    "# Update vllm.sh with correct model path\n",
    "import os\n",
    "import re\n",
    "\n",
    "vllm_script = \"vllm-server/vllm.sh\"\n",
    "with open(vllm_script, 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Update MODEL_PATH in script using regex (works even if already modified)\n",
    "content = re.sub(\n",
    "    r'MODEL_PATH=\"[^\"]*\"',\n",
    "    f'MODEL_PATH=\"{MODEL_PATH}\"',\n",
    "    content\n",
    ")\n",
    "\n",
    "with open(vllm_script, 'w') as f:\n",
    "    f.write(content)\n",
    "\n",
    "print(f\" Updated vllm.sh with model path: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server starting (PID: 182192)\n",
      "Check logs: tail -f vllm-server/vllm-server.log\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd vllm-server\n",
    "\n",
    "# Kill existing server\n",
    "pkill -f 'vllm.entrypoints.openai.api_server' || true\n",
    "sleep 2\n",
    "\n",
    "# Start in background with nohup\n",
    "nohup bash vllm.sh > vllm-server.log 2>&1 &\n",
    "\n",
    "echo \"Server starting (PID: $!)\"\n",
    "echo \"Check logs: tail -f vllm-server/vllm-server.log\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**While waiting for server startup:**\n",
    "\n",
    "You can monitor the server logs in the terminal:\n",
    "```bash\n",
    "tail -f /home/ubuntu/environment/neuron-workshops/labs/vLLM/Chess/assets/vllm-server/vllm-server.log\n",
    "```\n",
    "\n",
    "Or check Neuron core usage:\n",
    "```bash\n",
    "watch -n 2 neuron-ls\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Server Connection\n",
    "\n",
    "Once the server has started, let's test the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for server to be fully ready...\n",
      " vLLM server is running!\n",
      "\n",
      "Test response: Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import openai\n",
    "\n",
    "# Give server a moment to finish startup\n",
    "print(\"Waiting for server to be fully ready...\")\n",
    "time.sleep(10)\n",
    "\n",
    "# Initialize OpenAI client pointing to vLLM server\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"http://localhost:8080/v1\",\n",
    "    api_key=\"not-needed\"  # vLLM doesn't require authentication\n",
    ")\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_PATH,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "        max_tokens=10,\n",
    "        extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}}\n",
    "    )\n",
    "    print(\" vLLM server is running!\")\n",
    "    print(f\"\\nTest response: {response.choices[0].message.content}\")\n",
    "except Exception as e:\n",
    "    print(f\" Connection failed: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Check if server is still starting (check logs)\")\n",
    "    print(\"2. Verify Neuron cores are not in use: neuron-ls\")\n",
    "    print(\"3. Check for errors: tail vllm-server/vllm-server.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test Model Against Stockfish\n",
    "\n",
    "Now let's test against a real chess engine to get a sense of the model's strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Stockfish binary not found. Please install Stockfish or set the STOCKFISH_PATH environment variable.\n\nInstallation instructions:\n  macOS: brew install stockfish\n  Ubuntu/Debian: sudo apt install stockfish\n  Windows: Download from https://stockfishchess.org/download/\n  Or set STOCKFISH_PATH environment variable to point to your Stockfish binary.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StockfishAgent\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create Stockfish opponent (skill level 1 = beginner)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m stockfish \u001b[38;5;241m=\u001b[39m \u001b[43mStockfishAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mskill_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create environment\u001b[39;00m\n\u001b[1;32m      7\u001b[0m env \u001b[38;5;241m=\u001b[39m ChessEnvironment(\n\u001b[1;32m      8\u001b[0m     white_agent\u001b[38;5;241m=\u001b[39mvllm_agent,\n\u001b[1;32m      9\u001b[0m     black_agent\u001b[38;5;241m=\u001b[39mstockfish,\n\u001b[1;32m     10\u001b[0m     max_moves\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m~/environment/neuron-workshops/labs/vLLM/Chess/assets/agents/stockfish_agent.py:99\u001b[0m, in \u001b[0;36mStockfishAgent.__init__\u001b[0;34m(self, stockfish_path, depth, skill_level, elo_rating, parameters, time_limit_ms, hash_size_mb, threads)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreads \u001b[38;5;241m=\u001b[39m threads\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Find Stockfish binary\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstockfish_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_stockfish_binary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstockfish_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Initialize Stockfish process\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stockfish \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/environment/neuron-workshops/labs/vLLM/Chess/assets/agents/stockfish_agent.py:177\u001b[0m, in \u001b[0;36mStockfishAgent._find_stockfish_binary\u001b[0;34m(self, custom_path)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# If we get here, Stockfish was not found\u001b[39;00m\n\u001b[1;32m    168\u001b[0m error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStockfish binary not found. Please install Stockfish or set the STOCKFISH_PATH \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvironment variable.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Or set STOCKFISH_PATH environment variable to point to your Stockfish binary.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    176\u001b[0m )\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error_msg)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Stockfish binary not found. Please install Stockfish or set the STOCKFISH_PATH environment variable.\n\nInstallation instructions:\n  macOS: brew install stockfish\n  Ubuntu/Debian: sudo apt install stockfish\n  Windows: Download from https://stockfishchess.org/download/\n  Or set STOCKFISH_PATH environment variable to point to your Stockfish binary."
     ]
    }
   ],
   "source": [
    "from agents import StockfishAgent\n",
    "\n",
    "# Create Stockfish opponent (skill level 1 = beginner)\n",
    "stockfish = StockfishAgent(skill_level=1, depth=2)\n",
    "\n",
    "# Create environment\n",
    "env = ChessEnvironment(\n",
    "    white_agent=vllm_agent,\n",
    "    black_agent=stockfish,\n",
    "    max_moves=100\n",
    ")\n",
    "\n",
    "# Play game\n",
    "print(\"Playing chess game: vLLM vs Stockfish (skill=1)\\n\")\n",
    "result = env.play_game(verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Game Result\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Result: {result['result']}\")\n",
    "print(f\"Moves played: {result['moves_played']}\")\n",
    "print(f\"Reason: {result['game_over_reason']}\")\n",
    "print(f\"\\nYour model's illegal move attempts: {result['white_illegal_attempts']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Validate Performance\n",
    "\n",
    "Let's measure single-move latency to understand performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import time\n",
    "\n",
    "# Create a fresh board\n",
    "board = chess.Board()\n",
    "legal_moves = list(board.legal_moves)\n",
    "\n",
    "# Measure latency over 10 moves\n",
    "latencies = []\n",
    "print(\"Measuring latency over 10 moves...\\n\")\n",
    "\n",
    "for i in range(10):\n",
    "    start = time.time()\n",
    "    move, comment = vllm_agent.choose_move(\n",
    "        board=board.copy(),\n",
    "        legal_moves=legal_moves,\n",
    "        move_history=[],\n",
    "        side_to_move=\"White\"\n",
    "    )\n",
    "    latency = time.time() - start\n",
    "    latencies.append(latency)\n",
    "    print(f\"Move {i+1}: {latency:.3f}s\")\n",
    "\n",
    "# Calculate statistics\n",
    "import statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Performance Metrics\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean latency: {statistics.mean(latencies):.3f}s\")\n",
    "print(f\"Median latency: {statistics.median(latencies):.3f}s\")\n",
    "print(f\"Min latency: {min(latencies):.3f}s\")\n",
    "print(f\"Max latency: {max(latencies):.3f}s\")\n",
    "print(f\"Throughput: {1/statistics.mean(latencies):.2f} moves/second\")\n",
    "\n",
    "# Expected performance\n",
    "expected_latency = 0.65  # seconds\n",
    "if statistics.mean(latencies) < expected_latency * 1.2:\n",
    "    print(\"\\n Performance looks good!\")\n",
    "else:\n",
    "    print(f\"\\n  Latency higher than expected (~{expected_latency}s). Check server configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Concurrency Benefits\n",
    "\n",
    "Your vLLM server is configured with:\n",
    "- **`max_num_seqs=4`**: Can batch up to 4 requests\n",
    "- **`continuous_batching=true`**: Automatically groups concurrent requests\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "When you run multiple games in parallel (covered in Chess-Tournament.ipynb), the tournament scheduler uses Python multiprocessing to run games concurrently. Each game makes HTTP requests to the same vLLM server.\n",
    "\n",
    "**With 1 game (sequential):**\n",
    "- Latency: ~0.65s per move\n",
    "- Throughput: 1.54 moves/second\n",
    "\n",
    "**With 4 games (parallel):**\n",
    "- Latency per game: ~1.86s per move (games wait in small batches)\n",
    "- Total throughput: 2.15 moves/second across all games\n",
    "- **Improvement: 1.4x faster** (games complete ~40% faster overall)\n",
    "\n",
    "**Why the improvement?**\n",
    "The vLLM server batches concurrent requests together, processing them simultaneously on the Neuron cores. This amortizes the overhead of model inference across multiple requests.\n",
    "\n",
    "### Testing Concurrency:\n",
    "\n",
    "You'll see this in action in the next notebook (Chess-Tournament.ipynb), where running with `--parallelism 4` enables automatic batching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "\n",
    "- Deployed a chess model via vLLM on Trainium  \n",
    "- Configured vLLM for batching (max_num_seqs=4, continuous_batching)  \n",
    "- Tested the model with interactive games  \n",
    "- Validated performance metrics  \n",
    "- Understood concurrency benefits (1.4x throughput improvement)\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "Continue to [Chess-Tournament.ipynb](Chess-Tournament.ipynb) to:\n",
    "- Run competitive tournaments with TrueSkill ratings\n",
    "- Evaluate model strength against various Stockfish levels\n",
    "- See automatic batching in action with parallel games\n",
    "- Analyze detailed performance metrics\n",
    "\n",
    "**Keep the vLLM server running** for the next notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuronx_venv_pytorch_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
