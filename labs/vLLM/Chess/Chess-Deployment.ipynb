{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chess Model Deployment with vLLM on Trainium\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates deploying a fine-tuned chess model using vLLM on AWS Trainium. You'll learn how to:\n",
    "\n",
    "- Set up vLLM with Neuron backend for Trainium\n",
    "- Deploy a compiled chess model with batch support\n",
    "- Test the deployed model with interactive games\n",
    "- Validate performance and understand concurrency benefits\n",
    "\n",
    "**Prerequisites:**\n",
    "- Complete the fine-tuning lab or use a pre-trained chess model\n",
    "- Trainium instance (trn1.2xlarge or larger)\n",
    "\n",
    "**Duration:** 30-40 minutes (including model compilation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "First, let's install the chess environment and its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ubuntu/environment/neuron-workshops/labs/vLLM/Chess/assets\n",
    "\n",
    "# Install chess environment dependencies\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "print(\"\\n Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Upgrade to neuronx-cc 2.21.x\n",
    " !pip install --upgrade neuronx-cc==2.21.* --extra-index-url=https://pip.repos.neuron.amazonaws.com\n",
    " # Verify\n",
    " !neuronx-cc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Check Neuron Hardware\n",
    "\n",
    "Verify Neuron cores are available and not in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!neuron-ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:** You should see 2 Neuron cores available for trn1.2xlarge or 4 Neuron cores available for trn2.3xlarge\n",
    "If cores are in use, you can free them with: `!pkill -f vllm` or `kill pid`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configure Model Path\n",
    "\n",
    "Set the path to your fine-tuned chess model. If you completed the fine-tuning lab, update the `FINETUNED_MODEL_PATH` below.\n",
    "\n",
    "Otherwise, we'll use a pre-compiled model from the repository that we set with the HuggingFace `PRECOMPILED_MODEL_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Option 1: Use your fine-tuned model (update path if you completed fine-tuning lab)\n",
    "FINE_TUNED_MODEL_PATH = \"/home/ubuntu/environment/ml/qwen-chess/compiled_model\" # default path from previous lab\n",
    "\n",
    "# Option 2: Use pre-compiled model from HF repository (default), this will be pulled down by optimum-neuron-vllm\n",
    "PRECOMPILED_MODEL_PATH = \"kunhunjon/ChessLM_Qwen3_Trainium_AWS_Format\"\n",
    "\n",
    "# Verify fine-tuned model exists\n",
    "if os.path.exists(FINE_TUNED_MODEL_PATH):\n",
    "    print(f\" Fine-tuned Model found at: {FINE_TUNED_MODEL_PATH}\")\n",
    "    \n",
    "    # Check if fine-tuned model is compiled\n",
    "    if os.path.exists(os.path.join(FINE_TUNED_MODEL_PATH, \"model.pt\")):\n",
    "        print(\" Fine-tuned model found (model.pt)\")\n",
    "        \n",
    "        # Check model's neuron_config\n",
    "        import json\n",
    "        with open(os.path.join(FINE_TUNED_MODEL_PATH, \"neuron_config.json\")) as f:\n",
    "            config = json.load(f)\n",
    "        print(f\"   Batch size: {config.get('batch_size', 'N/A')}\")\n",
    "        print(f\"   Continuous batching: {config.get('continuous_batching', 'N/A')}\")\n",
    "        print(f\"   Tensor parallel: {config.get('tp_degree', 'N/A')}\")\n",
    "    else:\n",
    "        print(\"Model not compiled yet. Will compile on first run (10-30 min).\")\n",
    "else:\n",
    "    print(f\" Model not found at: {FINE_TUNED_MODEL_PATH}\")\n",
    "    print(\"Please update MODEL_PATH or complete the fine-tuning lab first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the model you wish to move forward with in this lab, update the MODEL_PATH variable below\n",
    "MODEL_PATH=PRECOMPILED_MODEL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Start vLLM Server\n",
    "\n",
    "Now let's start the vLLM server with Neuron backend. The precompiled model is configured for:\n",
    "- **batch_size=4**: Can process up to 4 requests concurrently\n",
    "- **continuous_batching=true**: Automatically groups concurrent requests\n",
    "- **tensor_parallel_size=2**: \n",
    "\n",
    "The fine-tuned model we just compiled is also configured for:\n",
    "- **batch_size=4**: Can process up to 4 requests concurrently\n",
    "- **continuous_batching=true**: Automatically groups concurrent requests\n",
    "- **tensor_parallel_size=2**:  \n",
    "\n",
    "**Note:** First-time startup includes model compilation which takes 10-30 minutes. Subsequent starts are fast (<1 min)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update vllm.sh with correct model path\n",
    "import os\n",
    "import re\n",
    "\n",
    "vllm_script = \"vllm-server/vllm.sh\"\n",
    "with open(vllm_script, 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Update MODEL_PATH in script using regex (works even if already modified)\n",
    "content = re.sub(\n",
    "    r'MODEL_PATH=\"[^\"]*\"',\n",
    "    f'MODEL_PATH=\"{MODEL_PATH}\"',\n",
    "    content\n",
    ")\n",
    "\n",
    "with open(vllm_script, 'w') as f:\n",
    "    f.write(content)\n",
    "\n",
    "print(f\" Updated vllm.sh with model path: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd vllm-server\n",
    "\n",
    "# Kill existing server\n",
    "pkill -f 'vllm.entrypoints.openai.api_server' || true\n",
    "sleep 2\n",
    "\n",
    "# Start in background with nohup\n",
    "nohup bash vllm.sh > vllm-server.log 2>&1 &\n",
    "\n",
    "echo \"Server starting (PID: $!)\"\n",
    "echo \"Check logs: tail -f vllm-server/vllm-server.log\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**While waiting for server startup:**\n",
    "\n",
    "You can monitor the server logs in the terminal:\n",
    "```bash\n",
    "tail -f /home/ubuntu/environment/neuron-workshops/labs/vLLM/Chess/assets/vllm-server/vllm-server.log\n",
    "```\n",
    "\n",
    "Or check Neuron core usage:\n",
    "```bash\n",
    "watch -n 2 neuron-ls\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Server Connection\n",
    "\n",
    "Once the server has started, let's test the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import openai\n",
    "\n",
    "# Give server a moment to finish startup\n",
    "print(\"Waiting for server to be fully ready...\")\n",
    "time.sleep(10)\n",
    "\n",
    "# Initialize OpenAI client pointing to vLLM server\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"http://localhost:8080/v1\",\n",
    "    api_key=\"not-needed\"  # vLLM doesn't require authentication\n",
    ")\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_PATH,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "        max_tokens=10,\n",
    "        extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}}\n",
    "    )\n",
    "    print(\" vLLM server is running!\")\n",
    "    print(f\"\\nTest response: {response.choices[0].message.content}\")\n",
    "except Exception as e:\n",
    "    print(f\" Connection failed: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Check if server is still starting (check logs)\")\n",
    "    print(\"2. Verify Neuron cores are not in use: neuron-ls\")\n",
    "    print(\"3. Check for errors: tail vllm-server/vllm-server.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test Model Against Stockfish\n",
    "\n",
    "Now let's test against a real chess engine to get a sense of the model's strength."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: prior to running any games, ensure your `.env` file is properly configured for vLLM and game parameters, or pass this to the agents directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ubuntu/environment/neuron-workshops/labs/vLLM/Chess/\n",
    "\n",
    "from assets.agents.vllm_agent import VLLMAgent\n",
    "from assets.agents.stockfish_agent import StockfishAgent\n",
    "from assets.env import ChessEnvironment\n",
    "\n",
    "# Create Stockfish opponent (skill level 1 = beginner)\n",
    "stockfish = StockfishAgent(skill_level=1, depth=2)\n",
    "\n",
    "# Create vLLM agent\n",
    "MODEL_PATH = \"kunhunjon/ChessLM_Qwen3_Trainium_AWS_Format\"\n",
    "BASE_URL = \"http://localhost:8080/v1\"\n",
    "\n",
    "# vLLM agent using the SAME config as your working test\n",
    "vllm_agent = VLLMAgent(\n",
    "    base_url=BASE_URL,\n",
    "    model=MODEL_PATH,\n",
    "    temperature=0.1,\n",
    "    max_tokens=200,\n",
    "    timeout=30.0,\n",
    "    retry_attempts=1,  \n",
    "    retry_delay=0.5,\n",
    ")\n",
    "\n",
    "# Create environment\n",
    "env = ChessEnvironment(\n",
    "    agent1=vllm_agent,\n",
    "    agent2=stockfish,\n",
    "    max_moves=100\n",
    ")\n",
    "\n",
    "# Play game\n",
    "print(\"Playing chess game: vLLM vs Stockfish (skill=1)\\n\")\n",
    "result = env.play_game(verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Game Result\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Result: {result['result']}\")\n",
    "print(f\"Moves played: {result['moves_played']}\")\n",
    "print(f\"Reason: {result['game_over_reason']}\")\n",
    "print(f\"\\nYour model's illegal move attempts: {result['white_illegal_attempts']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Validate Performance\n",
    "\n",
    "Let's measure single-move latency to understand performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import time\n",
    "\n",
    "# Create a fresh board\n",
    "board = chess.Board()\n",
    "legal_moves = list(board.legal_moves)\n",
    "\n",
    "# Measure latency over 10 moves\n",
    "latencies = []\n",
    "print(\"Measuring latency over 10 moves...\\n\")\n",
    "\n",
    "for i in range(10):\n",
    "    start = time.time()\n",
    "    move, comment = vllm_agent.choose_move(\n",
    "        board=board.copy(),\n",
    "        legal_moves=legal_moves,\n",
    "        move_history=[],\n",
    "        side_to_move=\"White\"\n",
    "    )\n",
    "    latency = time.time() - start\n",
    "    latencies.append(latency)\n",
    "    print(f\"Move {i+1}: {latency:.3f}s\")\n",
    "\n",
    "# Calculate statistics\n",
    "import statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Performance Metrics\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean latency: {statistics.mean(latencies):.3f}s\")\n",
    "print(f\"Median latency: {statistics.median(latencies):.3f}s\")\n",
    "print(f\"Min latency: {min(latencies):.3f}s\")\n",
    "print(f\"Max latency: {max(latencies):.3f}s\")\n",
    "print(f\"Throughput: {1/statistics.mean(latencies):.2f} moves/second\")\n",
    "\n",
    "# Expected performance\n",
    "expected_latency = 0.65  # seconds\n",
    "if statistics.mean(latencies) < expected_latency * 1.2:\n",
    "    print(\"\\n Performance looks good!\")\n",
    "else:\n",
    "    print(f\"\\n  Latency higher than expected (~{expected_latency}s). Check server configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Concurrency Benefits\n",
    "\n",
    "Your vLLM server is configured with:\n",
    "- **`max_num_seqs=4`**: Can batch up to 4 requests\n",
    "- **`continuous_batching=true`**: Automatically groups concurrent requests\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "When you run multiple games in parallel (covered in Chess-Tournament.ipynb), the tournament scheduler uses Python multiprocessing to run games concurrently. Each game makes HTTP requests to the same vLLM server.\n",
    "\n",
    "**With 1 game (sequential):**\n",
    "- Latency: ~0.65s per move\n",
    "- Throughput: 1.54 moves/second\n",
    "\n",
    "**With 4 games (parallel):**\n",
    "- Latency per game: ~1.86s per move (games wait in small batches)\n",
    "- Total throughput: 2.15 moves/second across all games\n",
    "- **Improvement: 1.4x faster** (games complete ~40% faster overall)\n",
    "\n",
    "**Why the improvement?**\n",
    "The vLLM server batches concurrent requests together, processing them simultaneously on the Neuron cores. This amortizes the overhead of model inference across multiple requests.\n",
    "\n",
    "### Testing Concurrency:\n",
    "\n",
    "You'll see this in action in the next notebook (Chess-Tournament.ipynb), where running with `--parallelism 4` enables automatic batching.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Your Games on Chess.com Analysis\n",
    "\n",
    "Once your games have been exported to PGN (e.g. `tournament_out/pgns/...` or `games.pgn`), you can replay and analyze them on Chess.com:\n",
    "\n",
    "1. Open the `.pgn` file in a text editor (VS Code, Notepad, nano, etc.).\n",
    "2. Select **one full game’s PGN** (from the `[Event \"...\"]` header down through the final result like `1-0`, `0-1`, or `1/2-1/2`).\n",
    "3. Copy that text to your clipboard.\n",
    "4. Go to [chess.com/analysis](https://www.chess.com/analysis).\n",
    "5. Click **“Import Game”** / **“Import PGN”** (button name may vary slightly).\n",
    "6. Paste the PGN into the text box.\n",
    "7. Click **“Import”** / **“Analyze”**.\n",
    "\n",
    "You’ll now see your game on an interactive board where you can:\n",
    "\n",
    "- Step through each move\n",
    "- Turn on engine analysis\n",
    "- Check evaluation bars, blunders, and alternative lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "\n",
    "- Deployed a chess model via vLLM on Trainium  \n",
    "- Configured vLLM for batching (max_num_seqs=4, continuous_batching)  \n",
    "- Tested the model with interactive games  \n",
    "- Validated performance metrics  \n",
    "- Understood concurrency benefits (1.4x throughput improvement)\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "Continue to [Chess-Tournament.ipynb](Chess-Tournament.ipynb) to:\n",
    "- Run competitive tournaments with TrueSkill ratings\n",
    "- Evaluate model strength against various Stockfish levels\n",
    "- See automatic batching in action with parallel games\n",
    "- Analyze detailed performance metrics\n",
    "\n",
    "**Keep the vLLM server running** for the next notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuronx_venv_pytorch_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
