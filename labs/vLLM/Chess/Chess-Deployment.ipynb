{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chess Model Deployment with vLLM on Trainium\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates deploying a fine-tuned chess model using vLLM on AWS Trainium. You'll learn how to:\n",
    "\n",
    "- Set up vLLM with Neuron backend for Trainium\n",
    "- Deploy a compiled chess model with batch support\n",
    "- Test the deployed model with interactive games\n",
    "- Validate performance and understand concurrency benefits\n",
    "\n",
    "**Prerequisites:**\n",
    "- Complete the fine-tuning lab or use a pre-trained chess model\n",
    "- Trainium instance (trn1.2xlarge or trn2.3xlarge)\n",
    "\n",
    "**Duration:** 30-40 minutes (including model compilation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "First, let's install the chess environment and its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/environment/neuron-workshops/labs/vLLM/Chess/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "âœ… Dependencies installed successfully!\n"
     ]
    }
   ],
   "source": [
    "%cd /home/ubuntu/environment/neuron-workshops/labs/vLLM/Chess/assets\n",
    "\n",
    "# Install chess environment dependencies\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "print(\"\\n Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Check Neuron Hardware\n",
    "\n",
    "Verify Neuron cores are available and not in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance-type: trn1.2xlarge\n",
      "instance-id: i-039d7490fb75ae0e2\n",
      "+--------+--------+----------+--------+--------------+----------+------+\n",
      "| NEURON | NEURON |  NEURON  | NEURON |     PCI      |   CPU    | NUMA |\n",
      "| DEVICE | CORES  | CORE IDS | MEMORY |     BDF      | AFFINITY | NODE |\n",
      "+--------+--------+----------+--------+--------------+----------+------+\n",
      "| 0      | 2      | 0-1      | 32 GB  | 0000:00:1e.0 | 0-7      | -1   |\n",
      "+--------+--------+----------+--------+--------------+----------+------+\n"
     ]
    }
   ],
   "source": [
    "!neuron-ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:** You should see 2 Neuron cores available for trn1.2xlarge or 4 Neuron cores available for trn2.3xlarge\n",
    "If cores are in use, you can free them with: `!pkill -f vllm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configure Model Path\n",
    "\n",
    "Set the path to your fine-tuned chess model. If you completed the fine-tuning lab, update the path below.\n",
    "\n",
    "Otherwise, we'll use a pre-compiled model from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model found at: /home/ubuntu/chess-model-qwen\n",
      " Pre-compiled model found (model.pt)\n",
      "   Batch size: 4\n",
      "   Continuous batching: True\n",
      "   Tensor parallel: 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Option 1: Use your fine-tuned model (update path if you completed fine-tuning lab)\n",
    "# MODEL_PATH = \"/home/ubuntu/environment/ChessLM_Qwen3_Trainium\"\n",
    "\n",
    "# Option 2: Use pre-compiled model from repository (default)\n",
    "MODEL_PATH = \"/home/ubuntu/chess-model-qwen\"\n",
    "\n",
    "# Verify model exists\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(f\" Model found at: {MODEL_PATH}\")\n",
    "    \n",
    "    # Check if model is compiled\n",
    "    if os.path.exists(os.path.join(MODEL_PATH, \"model.pt\")):\n",
    "        print(\" Pre-compiled model found (model.pt)\")\n",
    "        \n",
    "        # Check neuron_config\n",
    "        import json\n",
    "        with open(os.path.join(MODEL_PATH, \"neuron_config.json\")) as f:\n",
    "            config = json.load(f)\n",
    "        print(f\"   Batch size: {config.get('batch_size', 'N/A')}\")\n",
    "        print(f\"   Continuous batching: {config.get('continuous_batching', 'N/A')}\")\n",
    "        print(f\"   Tensor parallel: {config.get('tp_degree', 'N/A')}\")\n",
    "    else:\n",
    "        print(\"Model not compiled yet. Will compile on first run (10-30 min).\")\n",
    "else:\n",
    "    print(f\" Model not found at: {MODEL_PATH}\")\n",
    "    print(\"Please update MODEL_PATH or complete the fine-tuning lab first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Start vLLM Server\n",
    "\n",
    "Now let's start the vLLM server with Neuron backend. The server is configured for:\n",
    "- **batch_size=4**: Can process up to 4 requests concurrently\n",
    "- **continuous_batching=true**: Automatically groups concurrent requests\n",
    "- **tensor_parallel_size=2**: \n",
    "\n",
    "**Note:** First-time startup includes model compilation which takes 10-30 minutes. Subsequent starts are fast (<1 min)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Updated vllm.sh with model path: /home/ubuntu/chess-model-qwen\n"
     ]
    }
   ],
   "source": [
    "# Update vllm.sh with correct model path\n",
    "import os\n",
    "import re\n",
    "\n",
    "vllm_script = \"vllm-server/vllm.sh\"\n",
    "with open(vllm_script, 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Update MODEL_PATH in script using regex (works even if already modified)\n",
    "content = re.sub(\n",
    "    r'MODEL_PATH=\"[^\"]*\"',\n",
    "    f'MODEL_PATH=\"{MODEL_PATH}\"',\n",
    "    content\n",
    ")\n",
    "\n",
    "with open(vllm_script, 'w') as f:\n",
    "    f.write(content)\n",
    "\n",
    "print(f\" Updated vllm.sh with model path: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server starting (PID: 167816)\n",
      "Check logs: tail -f vllm-server/vllm-server.log\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd vllm-server\n",
    "\n",
    "# Kill existing server\n",
    "pkill -f 'vllm.entrypoints.openai.api_server' || true\n",
    "sleep 2\n",
    "\n",
    "# Start in background with nohup\n",
    "nohup bash vllm.sh > vllm-server.log 2>&1 &\n",
    "\n",
    "echo \"Server starting (PID: $!)\"\n",
    "echo \"Check logs: tail -f vllm-server/vllm-server.log\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**While waiting for server startup:**\n",
    "\n",
    "You can monitor the server logs in the terminal:\n",
    "```bash\n",
    "tail -f /home/ubuntu/environment/neuron-workshops/labs/vLLM/Chess/assets/vllm-server/vllm-server.log\n",
    "```\n",
    "\n",
    "Or check Neuron core usage:\n",
    "```bash\n",
    "watch -n 2 neuron-ls\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Server Connection\n",
    "\n",
    "Once the server has started, let's test the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for server to be fully ready...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Give server a moment to finish startup\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting for server to be fully ready...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Initialize OpenAI client pointing to vLLM server\u001b[39;00m\n\u001b[1;32m      9\u001b[0m client \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mOpenAI(\n\u001b[1;32m     10\u001b[0m     base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://localhost:8000/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot-needed\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# vLLM doesn't require authentication\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import openai\n",
    "\n",
    "# Give server a moment to finish startup\n",
    "print(\"Waiting for server to be fully ready...\")\n",
    "time.sleep(10)\n",
    "\n",
    "# Initialize OpenAI client pointing to vLLM server\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"not-needed\"  # vLLM doesn't require authentication\n",
    ")\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_PATH,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "        max_tokens=10,\n",
    "        extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}}\n",
    "    )\n",
    "    print(\" vLLM server is running!\")\n",
    "    print(f\"\\nTest response: {response.choices[0].message.content}\")\n",
    "except Exception as e:\n",
    "    print(f\" Connection failed: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Check if server is still starting (check logs)\")\n",
    "    print(\"2. Verify Neuron cores are not in use: neuron-ls\")\n",
    "    print(\"3. Check for errors: tail vllm-server/vllm-server.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Play Your First Chess Game\n",
    "\n",
    "Now that the server is running, let's play a quick game against a random opponent to verify everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import chess environment\n",
    "import sys\n",
    "sys.path.insert(0, '/home/ubuntu/environment/neuron-workshops/labs/vLLM/Chess/assets')\n",
    "\n",
    "from env import ChessEnvironment\n",
    "from agents import VLLMAgent, RandomAgent\n",
    "\n",
    "# Create agents\n",
    "vllm_agent = VLLMAgent(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    model=MODEL_PATH,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "random_agent = RandomAgent()\n",
    "\n",
    "# Create environment\n",
    "env = ChessEnvironment(\n",
    "    white_agent=vllm_agent,\n",
    "    black_agent=random_agent,\n",
    "    max_moves=100\n",
    ")\n",
    "\n",
    "# Play game\n",
    "print(\"Playing chess game: vLLM vs Random\\n\")\n",
    "result = env.play_game(verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Game Result\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Result: {result['result']}\")\n",
    "print(f\"Moves played: {result['moves_played']}\")\n",
    "print(f\"Reason: {result['game_over_reason']}\")\n",
    "print(f\"\\nWhite (vLLM) illegal attempts: {result['white_illegal_attempts']}\")\n",
    "print(f\"Black (Random) illegal attempts: {result['black_illegal_attempts']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test Against Stockfish\n",
    "\n",
    "Now let's test against a real chess engine to get a sense of the model's strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import StockfishAgent\n",
    "\n",
    "# Create Stockfish opponent (skill level 1 = beginner)\n",
    "stockfish = StockfishAgent(skill_level=1, depth=2)\n",
    "\n",
    "# Create environment\n",
    "env = ChessEnvironment(\n",
    "    white_agent=vllm_agent,\n",
    "    black_agent=stockfish,\n",
    "    max_moves=100\n",
    ")\n",
    "\n",
    "# Play game\n",
    "print(\"Playing chess game: vLLM vs Stockfish (skill=1)\\n\")\n",
    "result = env.play_game(verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Game Result\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Result: {result['result']}\")\n",
    "print(f\"Moves played: {result['moves_played']}\")\n",
    "print(f\"Reason: {result['game_over_reason']}\")\n",
    "print(f\"\\nYour model's illegal move attempts: {result['white_illegal_attempts']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Validate Performance\n",
    "\n",
    "Let's measure single-move latency to understand performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import time\n",
    "\n",
    "# Create a fresh board\n",
    "board = chess.Board()\n",
    "legal_moves = list(board.legal_moves)\n",
    "\n",
    "# Measure latency over 10 moves\n",
    "latencies = []\n",
    "print(\"Measuring latency over 10 moves...\\n\")\n",
    "\n",
    "for i in range(10):\n",
    "    start = time.time()\n",
    "    move, comment = vllm_agent.choose_move(\n",
    "        board=board.copy(),\n",
    "        legal_moves=legal_moves,\n",
    "        move_history=[],\n",
    "        side_to_move=\"White\"\n",
    "    )\n",
    "    latency = time.time() - start\n",
    "    latencies.append(latency)\n",
    "    print(f\"Move {i+1}: {latency:.3f}s\")\n",
    "\n",
    "# Calculate statistics\n",
    "import statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Performance Metrics\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean latency: {statistics.mean(latencies):.3f}s\")\n",
    "print(f\"Median latency: {statistics.median(latencies):.3f}s\")\n",
    "print(f\"Min latency: {min(latencies):.3f}s\")\n",
    "print(f\"Max latency: {max(latencies):.3f}s\")\n",
    "print(f\"Throughput: {1/statistics.mean(latencies):.2f} moves/second\")\n",
    "\n",
    "# Expected performance\n",
    "expected_latency = 0.65  # seconds\n",
    "if statistics.mean(latencies) < expected_latency * 1.2:\n",
    "    print(\"\\n Performance looks good!\")\n",
    "else:\n",
    "    print(f\"\\n  Latency higher than expected (~{expected_latency}s). Check server configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Concurrency Benefits\n",
    "\n",
    "Your vLLM server is configured with:\n",
    "- **`max_num_seqs=4`**: Can batch up to 4 requests\n",
    "- **`continuous_batching=true`**: Automatically groups concurrent requests\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "When you run multiple games in parallel (covered in Chess-Tournament.ipynb), the tournament scheduler uses Python multiprocessing to run games concurrently. Each game makes HTTP requests to the same vLLM server.\n",
    "\n",
    "**With 1 game (sequential):**\n",
    "- Latency: ~0.65s per move\n",
    "- Throughput: 1.54 moves/second\n",
    "\n",
    "**With 4 games (parallel):**\n",
    "- Latency per game: ~1.86s per move (games wait in small batches)\n",
    "- Total throughput: 2.15 moves/second across all games\n",
    "- **Improvement: 1.4x faster** (games complete ~40% faster overall)\n",
    "\n",
    "**Why the improvement?**\n",
    "The vLLM server batches concurrent requests together, processing them simultaneously on the Neuron cores. This amortizes the overhead of model inference across multiple requests.\n",
    "\n",
    "### Testing Concurrency:\n",
    "\n",
    "You'll see this in action in the next notebook (Chess-Tournament.ipynb), where running with `--parallelism 4` enables automatic batching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "\n",
    "- Deployed a chess model via vLLM on Trainium  \n",
    "- Configured vLLM for batching (max_num_seqs=4, continuous_batching)  \n",
    "- Tested the model with interactive games  \n",
    "- Validated performance metrics  \n",
    "- Understood concurrency benefits (1.4x throughput improvement)\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "Continue to [Chess-Tournament.ipynb](Chess-Tournament.ipynb) to:\n",
    "- Run competitive tournaments with TrueSkill ratings\n",
    "- Evaluate model strength against various Stockfish levels\n",
    "- See automatic batching in action with parallel games\n",
    "- Analyze detailed performance metrics\n",
    "\n",
    "**Keep the vLLM server running** for the next notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuronx_venv_pytorch_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
