{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chess Tournament Evaluation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates running competitive chess tournaments to evaluate your fine-tuned model. You'll learn how to:\n",
    "\n",
    "- Run tournaments with TrueSkill ratings\n",
    "- Leverage automatic request batching for throughput\n",
    "- Analyze model performance with detailed metrics\n",
    "- Compare against multiple Stockfish baselines\n",
    "\n",
    "**Prerequisites:**\n",
    "- Complete Chess-Deployment.ipynb\n",
    "- vLLM server running with your chess model\n",
    "\n",
    "**Duration:** 20-30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify vLLM Server is Running\n",
    "\n",
    "Before starting the tournament, let's verify the vLLM server is still running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "MODEL_PATH = \"kunhunjon/ChessLM_Qwen3_Trainium_AWS_Format\"\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"http://localhost:8080/v1\",\n",
    "    api_key=\"not-needed\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_PATH,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Test\"}],\n",
    "        max_tokens=5,\n",
    "        extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}}\n",
    "    )\n",
    "    print(\" vLLM server is running and ready!\")\n",
    "except Exception as e:\n",
    "    print(f\" Server not responding: {e}\")\n",
    "    print(\"\\nPlease start the server from Chess-Deployment.ipynb first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Play a Single Game\n",
    "\n",
    "Let's start with a single game to verify everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ubuntu/environment/neuron-workshops/labs/vLLM/Chess/\n",
    "\n",
    "# Play single game: vLLM vs Stockfish (skill 5)\n",
    "!python -m assets.run_game \\\n",
    "  --agent1 vllm \\\n",
    "  --agent2 stockfish-skill5-depth10 \\\n",
    "  --num-games 1 \\\n",
    "  --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run a Small Tournament (Sequential)\n",
    "\n",
    "Now let's run a small tournament with **parallelism=1** (sequential games) to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"Running 4 games sequentially (parallelism=1)...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "!python -m assets.run_game \\\n",
    "  --agent vllm \\\n",
    "  --agent stockfish-skill1-depth1 \\\n",
    "  --num-games 4 \\\n",
    "  --parallelism 1 \\\n",
    "  --output-dir tournament_sequential\n",
    "\n",
    "sequential_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n Sequential tournament completed in {sequential_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Tournament with Concurrency\n",
    "\n",
    "Now let's run the same tournament with **parallelism=4** to see the throughput improvement from automatic request batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running 4 games in parallel (parallelism=4)...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "!python -m assets.run_game \\\n",
    "  --agent vllm \\\n",
    "  --agent stockfish-skill1-depth1 \\\n",
    "  --num-games 4 \\\n",
    "  --parallelism 4 \\\n",
    "  --output-dir tournament_parallel\n",
    "\n",
    "parallel_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n Parallel tournament completed in {parallel_time:.1f} seconds\")\n",
    "print(f\"\\nSpeedup: {sequential_time/parallel_time:.2f}x faster with parallelism=4\")\n",
    "print(f\"Time saved: {sequential_time - parallel_time:.1f} seconds ({(sequential_time - parallel_time)/sequential_time*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Speedup\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. **Process-level parallelism** (`--parallelism 4`):\n",
    "   - Tournament scheduler runs 4 games simultaneously in separate processes\n",
    "   - Each game makes HTTP requests to the vLLM server independently\n",
    "\n",
    "2. **Request-level batching** (vLLM server):\n",
    "   - Server configured with `max_num_seqs=4` and `continuous_batching=true`\n",
    "   - When 4 games request moves at similar times, vLLM automatically batches them\n",
    "   - Batched requests are processed together on Neuron cores\n",
    "\n",
    "**Expected results:**\n",
    "- Sequential (parallelism=1): ~0.65s per move, games run one after another\n",
    "- Parallel (parallelism=4): ~1.86s per move, but 4 games run simultaneously\n",
    "- **Throughput improvement: ~1.4x** (40% faster overall)\n",
    "\n",
    "**Why not 4x speedup?**\n",
    "- Individual request latency increases due to batching overhead\n",
    "- Not all requests arrive at exactly the same time (timing variance)\n",
    "- Server batch efficiency: ~35% of theoretical maximum\n",
    "- Still significant savings: games complete much faster overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Full Tournament\n",
    "\n",
    "Now let's run a comprehensive tournament against multiple opponents to evaluate model strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m assets.run_game \\\n",
    "  --agent vllm \\\n",
    "  --agent stockfish-skill1-depth2 \\\n",
    "  --agent stockfish-skill5-depth10 \\\n",
    "  --agent stockfish-skill10-depth15 \\\n",
    "  --num-games 20 \\\n",
    "  --parallelism 4 \\\n",
    "  --output-dir tournament_full\n",
    "\n",
    "print(\"\\n Tournament complete! Results saved to tournament_full/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Analyze Results\n",
    "\n",
    "Let's load and analyze the tournament results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load tournament results\n",
    "with open('tournament_full/tournament.json') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "agents_data = []\n",
    "for agent_name, stats in results['agents'].items():\n",
    "    totals = stats['totals']\n",
    "    rating = stats['rating']\n",
    "\n",
    "    games  = totals.get('games', 0)\n",
    "    wins   = totals.get('wins', 0)\n",
    "    losses = totals.get('losses', 0)\n",
    "    draws  = totals.get('draws', 0)\n",
    "\n",
    "    win_rate = (wins + 0.5 * draws) / games * 100 if games > 0 else 0.0\n",
    "\n",
    "    agents_data.append({\n",
    "        'Agent': agent_name,\n",
    "        'Games': games,\n",
    "        'Wins': wins,\n",
    "        'Losses': losses,\n",
    "        'Draws': draws,\n",
    "        'Win Rate': f\"{win_rate:.1f}%\",\n",
    "        'Rating': f\"{rating['conservative']:.1f}\",\n",
    "        'Mu': f\"{rating['mu']:.2f}\",\n",
    "        'Sigma': f\"{rating['sigma']:.2f}\",\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(agents_data)\n",
    "df = df.sort_values('Rating', ascending=False)\n",
    "\n",
    "print(\"Tournament Standings:\")\n",
    "print(\"=\" * 80)\n",
    "print(df.to_string(index=False))\n",
    "print(\"\\nNote: Rating = mu - 3*sigma (conservative estimate)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Analyze Your Model's Performance\n",
    "\n",
    "Let's look at detailed metrics for your vLLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('tournament_full/tournament.json') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Get vLLM model statistics\n",
    "vllm_stats = results['agents']['vllm']\n",
    "totals = vllm_stats['totals']\n",
    "rating = vllm_stats['rating']\n",
    "engine = vllm_stats.get('engine_metrics_avg', {})\n",
    "illegal = vllm_stats.get('illegal_metrics', {})\n",
    "\n",
    "games  = totals.get('games', 0)\n",
    "wins   = totals.get('wins', 0)\n",
    "losses = totals.get('losses', 0)\n",
    "draws  = totals.get('draws', 0)\n",
    "as_white = totals.get('as_white', 0)\n",
    "as_black = totals.get('as_black', 0)\n",
    "\n",
    "win_rate = (wins + 0.5 * draws) / games * 100 if games > 0 else 0.0\n",
    "\n",
    "print(\"Your Model Performance:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nGames Played: {games}\")\n",
    "print(f\"Record: {wins}-{losses}-{draws}\")\n",
    "print(f\"Win Rate: {win_rate:.1f}%\")\n",
    "\n",
    "print(f\"\\nTrueSkill Rating:\")\n",
    "print(f\"  Mu (skill estimate): {rating['mu']:.2f}\")\n",
    "print(f\"  Sigma (uncertainty): {rating['sigma']:.2f}\")\n",
    "print(f\"  Conservative rating (mu - 3Ïƒ): {rating['conservative']:.1f}\")\n",
    "\n",
    "# Engine metrics (per-game averages from Stockfish analysis)\n",
    "if engine:\n",
    "    print(f\"\\nMove Quality (engine-based):\")\n",
    "    print(f\"  Accuracy: {engine.get('accuracy_pct', 0.0):.1f}% (matches Stockfish top move)\")\n",
    "    print(f\"  Avg Centipawn Loss: {engine.get('acpl', 0.0):.1f}\")\n",
    "\n",
    "# Illegal move stats\n",
    "if illegal:\n",
    "    print(f\"\\nIllegal Move Metrics:\")\n",
    "    print(f\"  Illegal attempts: {illegal.get('attempts', 0)}\")\n",
    "    print(f\"  Total move attempts: {illegal.get('move_attempts', 0)}\")\n",
    "    print(f\"  Illegal %: {illegal.get('illegal_pct', 0.0):.2f}%\")\n",
    "\n",
    "print(f\"\\nGames as White: {as_white}\")\n",
    "print(f\"Games as Black: {as_black}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Results\n",
    "\n",
    "**TrueSkill Rating:**\n",
    "- Conservative rating 15-20: ~1200-1400 ELO (beginner)\n",
    "- Conservative rating 20-25: ~1400-1600 ELO (intermediate)\n",
    "- Conservative rating 25-30: ~1600-1800 ELO (advanced)\n",
    "- Conservative rating 30+: ~1800+ ELO (expert)\n",
    "\n",
    "**Move Quality:**\n",
    "- Accuracy >60%: Model frequently finds Stockfish's top move\n",
    "- ACPL <50: Good move quality (average mistake < half a pawn)\n",
    "- ACPL <30: Excellent move quality (near-optimal play)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: View Sample Game\n",
    "\n",
    "Let's look at a sample game from the tournament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first game involving vLLM (either side)\n",
    "vllm_game = None\n",
    "for game in results['games']:\n",
    "    if 'vllm' in [game['white_agent_spec'], game['black_agent_spec']]:\n",
    "        vllm_game = game\n",
    "        break\n",
    "\n",
    "if vllm_game:\n",
    "    print(\"Sample Game:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Game ID: {vllm_game['id']}\")\n",
    "    print(f\"White: {vllm_game['white_agent_spec']}\")\n",
    "    print(f\"Black: {vllm_game['black_agent_spec']}\")\n",
    "    print(f\"Result: {vllm_game['result']}\")  # \"1-0\", \"0-1\", \"1-1\", or \"*\"\n",
    "    print(f\"Moves: {vllm_game['moves_played']}\")\n",
    "    print(f\"Reason: {vllm_game['game_over_reason']}\")\n",
    "    print(f\"Final FEN: {vllm_game['final_fen']}\")\n",
    "    \n",
    "    # PGN path on disk\n",
    "    pgn_path = vllm_game.get('pgn_path', '')\n",
    "    if pgn_path:\n",
    "        print(f\"\\nPGN file: {pgn_path}\")\n",
    "        print(\"You can open this file and paste the PGN into: https://lichess.org/paste\")\n",
    "    else:\n",
    "        print(\"\\nNo PGN path recorded for this game.\")\n",
    "else:\n",
    "    print(\"No vLLM games found in results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Compare Different Parallelism Levels\n",
    "\n",
    "Let's visualize how concurrency affects performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data comes from the concurrency benchmark we ran\n",
    "concurrency_data = {\n",
    "    'Concurrency': [1, 2, 4, 8],\n",
    "    'Throughput (req/s)': [1.53, 1.89, 2.15, 2.17],\n",
    "    'Mean Latency (s)': [0.653, 1.057, 1.860, 3.315],\n",
    "    'Speedup': [1.00, 1.23, 1.40, 1.42]\n",
    "}\n",
    "\n",
    "df_concurrency = pd.DataFrame(concurrency_data)\n",
    "\n",
    "print(\"Concurrency Performance:\")\n",
    "print(\"=\" * 70)\n",
    "print(df_concurrency.to_string(index=False))\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"- Best performance at parallelism=4 (matches server batch_size)\")\n",
    "print(\"- 1.4x throughput improvement with automatic batching\")\n",
    "print(\"- Individual latency increases, but total time decreases\")\n",
    "print(\"- Minimal benefit beyond parallelism=4 (server saturated)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "\n",
    "- Run chess tournaments with TrueSkill ratings  \n",
    "- Leveraged automatic request batching for 1.4x throughput  \n",
    "- Evaluated model strength against multiple baselines  \n",
    "- Analyzed performance with detailed metrics  \n",
    "- Understood concurrency trade-offs  \n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **Automatic Concurrency**: Process-level parallelism (`p_map`) + vLLM continuous batching work together automatically\n",
    "2. **Optimal Parallelism**: Set `--parallelism` to match `max_num_seqs` (typically 4) for best throughput\n",
    "3. **Throughput vs Latency**: Individual requests slower, but total throughput higher\n",
    "4. **Real-world Benefit**: Tournament games complete ~40% faster with parallelism=4\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- Fine-tune your model further based on tournament weaknesses\n",
    "- Experiment with different training datasets\n",
    "- Test against stronger opponents (Stockfish skill 15-20)\n",
    "- Deploy to production with learned configurations\n",
    "- Share your results with the community!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws_neuronx_venv_pytorch_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
